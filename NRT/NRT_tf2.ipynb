{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NRT-tf2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFYJcisY2iNu"
      },
      "source": [
        "#1. Below cell contains some constants\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RXk_nzwZwG"
      },
      "source": [
        "DIM_X = 300\n",
        "DIM_W = 300\n",
        "DIM_H = 400\n",
        "DIM_RATING = 6\n",
        "\n",
        "LEN_Y = 20 + 1\n",
        "\n",
        "LVT_DICT_SIZE = 8000\n",
        
        "\n",
        "W_EOS = \"<eos>\"\n",
        "W_UNK = \"<unk>\"\n",
        "W_START = \"<start>\"\n",
        "\n",
        
        "MIN_SUM_LEN = 5\n",
        "\n",
        
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6NuVSF2htN"
      },
      "source": [
        "#2. This cell is for preprocessing data change appropriate paths and corpus size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enewVxD6uKZo"
      },
      "source": [
        "#@title\n",
        "# -*- coding: utf-8 -*-\n",
        "#pylint: skip-file\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "import pickle\n",
        "import json\n",
        "import string\n",
        "import argparse\n",
        "import time\n",
        "import datetime\n",
        "#from commons import *\n",
        "import nltk\n",
        "import operator\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "stop_words_f = \"/content/drive/MyDrive/data/stopwords_en.txt\"\n",
        "\n",
        "def load_stop_words():\n",
        "    stop_words = {}\n",
        "    with open(stop_words_f) as f:\n",
        "        for line in f:\n",
        "            line = line.strip(\"\\n\")\n",
        "            stop_words[line] = 1\n",
        "    return stop_words\n",
        "\n",
        "def load_amazon():\n",
        "\n",
        "    LFW_T = 20 #3 for music, 20 for others\n",
        "\n",
        "    amazon_all = \"./data/amazon_review/aggressive_dedup.json\"\n",
        "    amazon_mini = \"./data/amazon_review/mini.json\"\n",
        "    amazon_core5 = \"./data/amazon_review/kcore_5.json\"\n",
        "    amazon_movie = \"./data/amazon_review/item_cats/reviews_Movies_and_TV_5.json\"\n",
        "    amazon_music = './data/Musical_Instruments_5.json'\n",
        "    amazon_data = '/content/drive/MyDrive/data/reviews_Electronics_5.json'\n",
        "\n",
        "    size_corpus = 1400000 #for electronics :1,689,188 . #for books :8,898,041. #82836502.    \n",
        "    stop_words = load_stop_words()\n",
        "\n",
        "    dic = {}\n",
        "    i2w = {}\n",
        "    w2i = {}\n",
        "    i2user = {}\n",
        "    user2i = {}\n",
        "    i2item = {}\n",
        "    item2i = {}\n",
        "    user2w = {}\n",
        "    item2w = {}\n",
        "    x_raw = []\n",
        "    timestamps = []\n",
        "    w2df = {}\n",
        " \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    with open(amazon_data) as f:\n",
        "        i = 0.\n",
        "        for line in f:\n",
        "            try:\n",
        "                line = line.strip(\"\\n\")\n",
        "                a = json.loads(line)\n",
        "                user_id = a[\"reviewerID\"]\n",
        "                item_id = a[\"asin\"]\n",
        "                review = a[\"reviewText\"].lower()\n",
        "                rating = a[\"overall\"]\n",
        "                summary = a[\"summary\"].lower()\n",
        "                unix_time = a[\"unixReviewTime\"]\n",
        "                raw_time = a[\"reviewTime\"]\n",
        "\n",
        "                if user_id not in user2i:\n",
        "                    user2i[user_id] = len(i2user)\n",
        "                    i2user[user2i[user_id]] = user_id\n",
        "                if item_id not in item2i:\n",
        "                    item2i[item_id] = len(i2item)\n",
        "                    i2item[item2i[item_id]] = item_id\n",
        "                \n",
        "                if len(summary.split()) < MIN_SUM_LEN:\n",
        "                    rev_sents = sent_tokenize(review)\n",
        "                    for sent in rev_sents:\n",
        "                        summary = sent\n",
        "                        if len(summary.split()) >= MIN_SUM_LEN:\n",
        "                            break\n",
        "                \n",
        "                if len(summary.split()) < MIN_SUM_LEN:\n",
        "                    continue\n",
        "\n",
        "                review = review.translate(translator)\n",
        "                terms = review.split()\n",
        "                review_words = []\n",
        "                for t in terms:\n",
        "                    if t in stop_words:\n",
        "                        continue\n",
        "                    review_words.append(t)\n",
        "                    if t in dic:\n",
        "                        dic[t] += 1\n",
        "                    else:\n",
        "                        dic[t] = 1\n",
        "                    \n",
        "                    if t in w2df:\n",
        "                        w2df[t].add(len(timestamps))\n",
        "                    else:\n",
        "                        w2df[t] = set([item_id])\n",
        "\n",
        "                summary = summary.translate(translator) \n",
        "                terms = summary.split() \n",
        "                for t in terms:\n",
        "                    if t in dic:\n",
        "                        dic[t] += 1\n",
        "                    else:\n",
        "                        dic[t] = 1\n",
        "\n",
        "                    if t in w2df:\n",
        "                        w2df[t].add(len(timestamps))\n",
        "                    else:\n",
        "                        w2df[t] = set([item_id])\n",
        "\n",
        "                x_raw.append([user2i[user_id], item2i[item_id], rating, terms, review_words, unix_time, raw_time])\n",
        "                timestamps.append(unix_time)\n",
        "            except KeyError:\n",
        "                print (\"ops: \" + line)\n",
        "            i += 1.\n",
        "            print ('\\r{0}'.format(i / size_corpus) + \" / 1 \",)\n",
        "            if i>1400000:\n",
        "              break\n",
        "    \n",
        "    timestamps.sort()\n",
        "    spliter1 = timestamps[int(len(timestamps) * 0.8)]\n",
        "    spliter2 = timestamps[int(len(timestamps) * 0.9)]\n",
        "    x_raw_train = []\n",
        "    x_raw_test = []\n",
        "    x_raw_dev = []\n",
        "    train_uid = {}\n",
        "    train_iid = {}\n",
        "    for i in range(len(x_raw)):\n",
        "        if x_raw[i][5] < spliter1:\n",
        "            x_raw_train.append(x_raw[i])\n",
        "            train_uid[x_raw[i][0]] = 1\n",
        "            train_iid[x_raw[i][1]] = 1\n",
        "        elif x_raw[i][5] < spliter2:\n",
        "            # test samples must be in train set\n",
        "            if (x_raw[i][0] in train_uid) and (x_raw[i][1] in train_iid):\n",
        "                x_raw_dev.append(x_raw[i])\n",
        "        else:\n",
        "            if (x_raw[i][0] in train_uid) and (x_raw[i][1] in train_iid):\n",
        "                x_raw_test.append(x_raw[i])\n",
        "    \n",
        "\n",
        "    new_dic = {}\n",
        "    w2i = {}\n",
        "    iw2 = {}\n",
        "    w2w = {}\n",
        "    hfw = []\n",
        "    new_w2df = {}\n",
        "    aq= [w2df[i]  for i in w2df if len(w2df[i])==1]\n",
        "    print(aq)\n",
        "    if W_EOS not in dic:\n",
        "        dic[W_EOS] = len(dic)\n",
        "        w2df[W_EOS] = set([1])\n",
        "    if W_UNK not in dic:\n",
        "        dic[W_UNK] = len(dic)\n",
        "        w2df[W_UNK] = set([1])\n",
        "\n",
        "    for w in dic:\n",
        "        if dic[w] >= LFW_T:\n",
        "            w2i[w] = len(new_dic)\n",
        "            i2w[w2i[w]] = w\n",
        "            new_dic[w] = dic[w]\n",
        "            w2w[w] = w\n",
        "            new_w2df[w] = np.log10(len(timestamps) / len(w2df[w])) \n",
        "        else:\n",
        "            w2w[w] = W_UNK\n",
        "\n",
        "    new_w2df[W_EOS] = 1e-15\n",
        "    new_w2df[W_UNK] = 1e-15\n",
        "\n",
        "    sorted_x = sorted(new_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for w in sorted_x:\n",
        "        hfw.append(w[0])\n",
        "\n",
        "    for data_item in x_raw_train:\n",
        "        [uid, iid, _, summ, review_words, _, _] = data_item\n",
        "        for w in (summ + review_words):\n",
        "            if uid in user2w:\n",
        "                user2w[uid].add(w2i[w2w[w]])\n",
        "            else:\n",
        "                user2w[uid] = {w2i[w2w[w]]}\n",
        "            if iid in item2w:\n",
        "                item2w[iid].add(w2i[w2w[w]])\n",
        "            else:\n",
        "                item2w[iid] = {w2i[w2w[w]]}\n",
        "\n",
        "    all_data = [x_raw_train, x_raw_dev, x_raw_test, dic, new_dic, hfw, i2w, w2i, w2w, new_w2df, i2user, user2i, i2item, item2i, user2w, item2w]\n",
        "\n",
        "    o = open('/content/drive/MyDrive/data/data_amazon_electronics.pkl', 'wb')\n",
        "    pickle.dump(all_data, o, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "    o.close()\n",
        "    \n",
        "    print (len(dic), len(new_dic), len(i2w), len(w2i), len(w2w))\n",
        "    print (\"#dic=\", len(dic), \"#new_dic=\", len(new_dic), \"#user=\", len(i2user), \"#item=\", len(i2item))\n",
        "    print (\"#x_raw=\", len(x_raw), \"#x_train=\", len(x_raw_train),\"#x_dev=\", len(x_raw_dev),  \"#x_test=\", len(x_raw_test))\n",
        "    x_raw = []\n",
        "\n",
        "    ###################################################\n",
        "    # movie\n",
        "    #dic= 2719934 #user= 123960 #item= 50052\n",
        "    #x_raw= 1697533 #x_train= 1357893 #x_test= 163529\n",
        "    ###################################################\n",
        "    # kcore-5\n",
        "    #dic= 17511889 #user= 3035045 #item= 1569973\n",
        "    #x_raw= 41135696 #x_train= 32906330 #x_test= 4928037\n",
        "    ###################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument(\"data\", help=\"which dataset will be processed\")\n",
        "    #args = parser.parse_args()\n",
        "    data = \"amazon\"\n",
        "    if data == \"amazon\":\n",
        "        load_amazon()\n",
        "    elif data == \"yelp\":\n",
        "        load_yelp()\n",
        "    elif data == \"yelp5\":\n",
        "        load_yelp5()\n",
        "    else:\n",
        "        print (\"error: data\")\n",
        "    print(MIN_SUM_LEN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MYOavzbbPOc"
      },
      "source": [
        "#3 Initialise modules function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99uORgfRz-bV"
      },
      "source": [
        "import sys\n",
        "import  pickle\n",
        "#from nrec import *\n",
        "#from utils_pg import *\n",
        "#import data as datar\n",
        "import math\n",
        "import time\n",
        "import argparse\n",
        "#from commons import *\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "#from os import makedirs\n",
        "#from os.path import exists\n",
        "\n",
        "\n",
        "\n",
        "def load_stopwords(f_path = None):\n",
        "    stop_words = {}\n",
        "    f = open(f_path, \"r\")\n",
        "    for line in f:\n",
        "        line = line.strip('\\n').lower()\n",
        "        stop_words[line] = 1\n",
        "    return stop_words\n",
        "\n",
        "def init_modules(data_type='amazon'):\n",
        "    options = {}\n",
        "   \n",
        "    options[\"is_predicting\"] = False\n",
        "\n",
        "    print (\"is_predicting: \", options[\"is_predicting\"], \", data = \" + data_type)\n",
        "\n",
        "    options[\"is_unicode\"] = True\n",
        "    \n",
        "    print (\"loading...\")\n",
        "    all_data = pickle.load(open(\"/content/drive/MyDrive/data/data_\"+ data_type + \"_electronics(2).pkl\", \"rb\"))\n",
        "    [x_raw_train, x_raw_dev, x_raw_test, old_dic, dic, hfw, i2w, w2i, w2w, w2idf, i2user, user2i, i2item, item2i, user2w, item2w] = all_data\n",
        "\n",
        "    consts = {}\n",
        "    consts[\"num_user\"] = len(user2i)\n",
        "    consts[\"num_item\"] = len(item2i)\n",
        "    consts[\"dict_size\"] = len(dic)+1\n",
        "    consts[\"hfw_dict_size\"] = len(hfw)+1\n",
        "    consts[\"lvt_dict_size\"] = LVT_DICT_SIZE+1\n",
        "    consts[\"len_y\"] = LEN_Y\n",
        "    if data_type == \"yelp\":\n",
        "        consts[\"len_y\"] = LEN_Y_YELP\n",
        "        consts[\"lvt_dict_size\"] = LVT_DICT_SIZE_YELP\n",
        "\n",
        "    consts[\"dim_x\"] = DIM_X\n",
        "    consts[\"dim_w\"] = DIM_W\n",
        "    consts[\"dim_h\"] = DIM_H\n",
        "    consts[\"dim_r\"] = DIM_RATING\n",
        "    consts[\"max_len_predict\"] = consts[\"len_y\"]\n",
        "    consts[\"min_len_predict\"] = 1\n",
        "\n",
        "    consts[\"batch_size\"] = 200\n",
        "    \n",
        "    consts[\"test_batch_size\"] = 1\n",
        "    consts[\"test_samples\"] = 2000\n",
        "\n",
        "\n",
        "    consts[\"beam_size\"] = 5\n",
        "\n",
        "    consts[\"lr\"] = 1.\n",
        "\n",
        "    consts[\"print_size\"] = 2#400 2 for demo\n",
        "\n",
        "    print (\"#train = \", len(x_raw_train), \", #dev = \", len(x_raw_dev), \", #test = \", len(x_raw_test))\n",
        "    print (\"#user = \", consts[\"num_user\"], \", #item = \", consts[\"num_item\"])\n",
        "    print (\"#dic = \", consts[\"dict_size\"], \", #hfw = \", consts[\"hfw_dict_size\"])\n",
        "\n",
        "    modules = {}\n",
        "    modules[\"all_data\"] = all_data\n",
        "    hfw.append('<start>')\n",
        "    modules[\"hfw\"] = hfw\n",
        "    w2i['<start>']=len(w2i)\n",
        "    modules[\"w2i\"] = w2i\n",
        "    \n",
        "    modules[\"i2w\"] = i2w\n",
        "    modules[\"w2w\"] = w2w\n",
        "    w2w['<start>'] = '<start>'\n",
        "    modules[\"u2ws\"] = user2w\n",
        "    modules[\"i2ws\"] = item2w\n",
        "    modules[\"w2idf\"] = w2idf\n",
        "\n",
        "    modules[\"eos_emb\"] = modules[\"w2i\"][W_EOS]\n",
        "    modules[\"unk_emb\"] = modules[\"w2i\"][W_UNK]\n",
        "    modules[\"<start>\"] = modules[\"w2i\"][W_START]\n",
        "    \n",
        "    modules[\"optimizer\"] = \"adadelta\"\n",
        "    \n",
        "    modules[\"stopwords\"] = load_stopwords(\"/content/drive/MyDrive/data/stopwords_en.txt\")\n",
        "\n",
        "    return modules, consts, options\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaPizU2Xcx_h"
      },
      "source": [
        "modules,consts,options = init_modules()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O0_Hq6bxAPX"
      },
      "source": [
        "#now create pipeline for data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fchAP2PUpZf8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#pylint: skip-file\n",
        "import sys\n",
        "import os\n",
        "import os.path\n",
        "import time\n",
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "#from commons import *\n",
        "\n",
        "\n",
        "def totensorf(arg):\n",
        "    arg = tf.convert_to_tensor(arg,dtype=tf.float32)\n",
        "    return arg\n",
        "def totensori(arg):\n",
        "    arg = tf.convert_to_tensor(arg,dtype=tf.int32)\n",
        "    return arg\n",
        "def get_batch_index(n, batch_size, is_predict = False):\n",
        "    i_list = [i for i in range(n)]\n",
        "    if not is_predict:\n",
        "        random.shuffle(i_list)\n",
        "\n",
        "    batch_index = []\n",
        "    x = []\n",
        "    for i in range(n):\n",
        "        x.append(i_list[i])\n",
        "        if len(x) == batch_size or i == (n-1):\n",
        "            batch_index.append(x)\n",
        "            x = []\n",
        "    return batch_index\n",
        "\n",
        "def get_batch_data(raw_x, consts, options, modules):\n",
        "    batch_size = len(raw_x)\n",
        "    hfw = modules[\"hfw\"]\n",
        "    w2i = modules[\"w2i\"]\n",
        "    i2w = modules[\"i2w\"]\n",
        "    w2w = modules[\"w2w\"]\n",
        "    w2idf = modules[\"w2idf\"]\n",
        "    u2ws = modules[\"u2ws\"]\n",
        "    i2ws = modules[\"i2ws\"]\n",
        "    len_y = consts[\"len_y\"]\n",
        "    lvt_dict_size = consts[\"lvt_dict_size\"]\n",
        "\n",
        "    xu = np.zeros(batch_size)\n",
        "    xi = np.zeros(batch_size)\n",
        "    yr = np.zeros(batch_size)\n",
        "    yr_vec = np.zeros((batch_size, DIM_RATING) )\n",
        "    y_review_mark = np.zeros((batch_size, lvt_dict_size))\n",
        "    y_review_tf = np.zeros((batch_size, lvt_dict_size) )\n",
        "    y_summary_index = np.zeros((len_y+1, batch_size, 1) ) #add <start> token\n",
        "    y_summary_mark = np.zeros((len_y+1, batch_size) )\n",
        "    y_summary_lvt = np.zeros((len_y+1, batch_size, 1) )\n",
        "\n",
        "    dic_review_words = {}\n",
        "    w2i_review_words = {}\n",
        "    lst_review_words = []\n",
        "    lvt_user_item = {w2i[W_EOS]}\n",
        "    lvt_i2i = {}\n",
        "    for i in range(batch_size):\n",
        "        uid, iid, r, summary, review, _, _ = raw_x[i] \n",
        "        xu[i] = uid\n",
        "        xi[i] = iid\n",
        "        yr[i] = r\n",
        "        yr_vec[i, int(r)] = 1.\n",
        "        lvt_user_item |= u2ws[uid] | i2ws[iid]\n",
        "\n",
        "        if len(summary) > len_y:\n",
        "            summary = summary[0:len_y-1] + [W_EOS]\n",
        "        summary = [W_START] + summary\n",
        "        for wi in range(len(summary)):\n",
        "            w = summary[wi]\n",
        "            w = w2w[w]\n",
        "            if w in dic_review_words:\n",
        "                dic_review_words[w] += 1\n",
        "            else:\n",
        "                w2i_review_words[w] = len(dic_review_words)\n",
        "                dic_review_words[w] = 1\n",
        "                lst_review_words.append(w2i[w])\n",
        "            y_summary_index[wi, i, 0] = w2i[w]\n",
        "            y_summary_lvt[wi, i, 0] = w2i_review_words[w]\n",
        "            y_summary_mark[wi, i] = 1\n",
        "\n",
        "        for w in review:\n",
        "            w = w2w[w]\n",
        "            if w in dic_review_words:\n",
        "                dic_review_words[w] += 1\n",
        "            else:\n",
        "                w2i_review_words[w] = len(dic_review_words)\n",
        "                dic_review_words[w] = 1\n",
        "                lst_review_words.append(w2i[w])\n",
        "            y_review_mark[i, w2i_review_words[w]] = 1\n",
        "            y_review_tf[i, w2i_review_words[w]] += 1#w2idf[w]\n",
        "\n",
        "    y_review_tf /= 10.\n",
        "\n",
        "    if len(dic_review_words) < lvt_dict_size:\n",
        "        for rd_hfw in hfw:\n",
        "            if rd_hfw not in dic_review_words:\n",
        "                w2i_review_words[rd_hfw] = len(dic_review_words)\n",
        "                dic_review_words[rd_hfw] = 0\n",
        "                lst_review_words.append(w2i[rd_hfw])\n",
        "            if len(dic_review_words) == lvt_dict_size:\n",
        "                break\n",
        "    else:\n",
        "        print (\"!!!!!!!!!!!!\")\n",
        "\n",
        "    for i in range(len(lst_review_words)):\n",
        "        lvt_i2i[i] = lst_review_words[i]\n",
        "    assert len(dic_review_words) == lvt_dict_size\n",
        "    assert len(lst_review_words) == lvt_dict_size\n",
        "\n",
        "    if options[\"is_predicting\"]:\n",
        "        lvt_i2i = {}\n",
        "        lst_review_words = list(lvt_user_item)\n",
        "        for i in range(len(lst_review_words)):\n",
        "            lvt_i2i[i] = lst_review_words[i]\n",
        "        \n",
        "    lst_review_words = np.asarray(lst_review_words)\n",
        "    #print(lvt_i2i)\n",
        "    #print(dic_review_words[\"adf\"])\n",
        "    xu = totensori(xu)\n",
        "    xi = totensori(xi)\n",
        "    yr = totensorf(yr)\n",
        "    yr_vec = totensorf(yr_vec)\n",
        "    y_review_mark = totensorf(y_review_mark)\n",
        "    y_review_tf = totensorf(y_review_tf)\n",
        "    y_summary_index = totensori(y_summary_index)\n",
        "    y_summary_mark = totensorf(y_summary_mark)\n",
        "    y_summary_lvt = totensori(y_summary_lvt)\n",
        "\n",
        "    return xu, xi, yr, yr_vec, lst_review_words, y_review_tf, y_review_mark, y_summary_index, y_summary_mark, y_summary_lvt, lvt_i2i\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spcl5Pi0lbtH"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuivQH2V6gCS"
      },
      "source": [
        "\n",
        "class RatingUser(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_h):\n",
        "    super(RatingUser,self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense2 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense3 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "  def call(self,x_emb,training=False):\n",
        "    hru = self.dense1(x_emb)\n",
        "    hru = self.dense2(hru)\n",
        "    hru = self.dense3(hru)\n",
        "    return hru\n",
        "\n",
        "\n",
        "class RatingItem(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_h):\n",
        "    super(RatingItem,self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense2 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense3 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "  def call(self,i_emb,training=False):\n",
        "    hri = self.dense1(i_emb)\n",
        "    hri = self.dense2(hri)\n",
        "    hri = self.dense3(hri)\n",
        "    return hri\n",
        "\n",
        "\n",
        "class Rating(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_h):\n",
        "    super(Rating,self).__init__()\n",
        "    self.userdense = tf.keras.layers.Dense(dim_h)\n",
        "    self.itemdense = tf.keras.layers.Dense(dim_h)\n",
        "    self.useritemdense = tf.keras.layers.Dense(dim_h)\n",
        "    self.sigmoid1 = tf.keras.activations.sigmoid\n",
        "    #self.dense1 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense2 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense3 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "  def call(self,x_emb,i_emb,training=False):\n",
        "    hru = self.userdense(x_emb)\n",
        "    hri = self.itemdense(i_emb)\n",
        "    hrui = self.useritemdense(x_emb*i_emb)\n",
        "    hr = self.sigmoid1(hru+hri+hrui)\n",
        "\n",
        "    hr = self.dense2(hr)\n",
        "    hr = self.dense3(hr)\n",
        "    return hr\n",
        "\n",
        "class RatingUserItem(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_h):\n",
        "    super(RatingUserItem,self).__init__()\n",
        "    self.userdense = tf.keras.layers.Dense(dim_h)\n",
        "    self.itemdense = tf.keras.layers.Dense(dim_h)\n",
        "    self.sigmoid1 = tf.keras.activations.sigmoid\n",
        "    self.dense2 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "    self.dense3 = tf.keras.layers.Dense(dim_h,activation = 'sigmoid')\n",
        "  def call(self,x_emb,i_emb,training=False):\n",
        "    hru = self.userdense(x_emb)\n",
        "    hri = self.itemdense(i_emb)\n",
        "    hrui = hru + hri\n",
        "    hrui = self.sigmoid1(hrui)\n",
        "    hrui = self.dense2(hrui)   \n",
        "    hrui = self.dense3(hrui)\n",
        "    return hrui\n",
        "\n",
        "\n",
        "\n",
        "class NRTrating(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NRTrating,self).__init__()\n",
        "    self.num_user = consts['num_user']\n",
        "    self.num_item = consts['num_item']\n",
        "    self.dim_x  = consts['dim_x']\n",
        "    self.dim_h = consts['dim_h']\n",
        "    self.dim_r = consts['dim_r']\n",
        "    self.dict_size = consts['hfw_dict_size']\n",
        "    self.lvt_dict_size = consts['lvt_dict_size']\n",
        "    \n",
        "    self.embeddingu = tf.keras.layers.Embedding(self.num_user, self.dim_x)\n",
        "    self.embeddingi = tf.keras.layers.Embedding(self.num_item, self.dim_x)\n",
        "    \n",
        "    self.ratinguser = RatingUser(self.dim_h)\n",
        "\n",
        "    self.ratingitem = RatingItem(self.dim_h)\n",
        "\n",
        "    self.ratinguseritem = RatingUserItem(self.dim_h)\n",
        "\n",
        "    self.rating = Rating(self.dim_h)\n",
        "\n",
        "    self.whru3 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.whri3 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.whr3 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.sigmoid1 = tf.keras.activations.sigmoid\n",
        "    self.wrhr = tf.keras.layers.Dense(1)\n",
        "    self.wembu = tf.keras.layers.Dense(1)\n",
        "    self.wembi = tf.keras.layers.Dense(1)\n",
        "    self.wreviewi = tf.keras.layers.Dense(self.lvt_dict_size)\n",
        "    self.wreviewu = tf.keras.layers.Dense(self.lvt_dict_size)\n",
        "    self.wreview = tf.keras.layers.Dense(self.lvt_dict_size)\n",
        "    self.softmax1 = tf.keras.activations.softmax\n",
        "    self.softmax2 = tf.keras.activations.softmax\n",
        "    self.softmax3 = tf.keras.activations.softmax\n",
        "    self.ddense1 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.ddense2 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.ddense3 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.ddense4 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.ddense5 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.ddense6 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.dtanh1 = tf.keras.activations.tanh\n",
        "\n",
        "\n",
        "  def call(self,XU,XI,emb_r,training=False):\n",
        "    x_emb = self.embeddingu(XU) #batch_size, dim_x\n",
        "    i_emb = self.embeddingi(XI)\n",
        "\n",
        "    hru3 = self.ratinguser(x_emb)\n",
        "    hri3 = self.ratingitem(i_emb)\n",
        "    hrui3 = self.ratinguseritem(x_emb,i_emb)\n",
        "    hr3 = self.rating(x_emb,i_emb)\n",
        "\n",
        "    hr4 = self.whru3(hru3)+self.whri3(hri3)+self.whr3(hr3)\n",
        "    hr4 = self.sigmoid1(hr4)\n",
        "\n",
        "    rating = self.wrhr(hr4)+self.wembu(x_emb)+self.wembi(i_emb)\n",
        "\n",
        "    review_i = self.wreviewi(hri3)\n",
        "    review_i = self.softmax1(review_i)\n",
        "    review_u = self.wreviewu(hru3)\n",
        "    review_u = self.softmax2(review_u)\n",
        "    review = self.wreview(hrui3)\n",
        "    review = self.softmax3(review)\n",
        "    h_s = self.dtanh1(self.ddense1(x_emb)+self.ddense2(i_emb)+self.ddense3(emb_r)/\n",
        "                     self.ddense4(hru3)+self.ddense5(hri3)+self.ddense6(hrui3))\n",
        "    return rating,review,review_i,review_u,x_emb,i_emb,hru3,hri3,hrui3,h_s\n",
        "\n",
        "\n",
        "class NRTdecoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NRTdecoder,self).__init__()\n",
        "    self.num_user = consts['num_user']\n",
        "    self.num_item = consts['num_item']\n",
        "    self.dim_x  = consts['dim_x']\n",
        "    self.dim_h = consts['dim_h']\n",
        "    self.dim_r = consts['dim_r']\n",
        "    self.dict_size = consts['hfw_dict_size']\n",
        "    self.lvt_dict_size = consts['lvt_dict_size']\n",
        "    self.dim_w = consts['dim_w']\n",
        "\n",
        "    self.embedding1 = tf.keras.layers.Embedding(self.dict_size,self.dim_w)\n",
        "    self.gru = tf.keras.layers.GRU(self.dim_h,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.dense1 = tf.keras.layers.Dense(self.dict_size)\n",
        "    self.dense2 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.dense3 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.dense4 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.dense5 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.dense6 = tf.keras.layers.Dense(self.dim_h)\n",
        "    self.tanh = tf.keras.activations.tanh\n",
        "    self.softmax = tf.keras.activations.softmax\n",
        "  def call(self,x,hidden,xu_emb,xi_emb,emb_r,training=False):\n",
        "    \n",
        "    y_emb = self.embedding1(x)\n",
        "\n",
        "    x  = tf.concat([tf.expand_dims(hidden, 1), y_emb], axis=-1)\n",
        "    #print(x.shape)\n",
        "    output,state = self.gru(x)\n",
        "\n",
        "    output = tf.reshape(output,(-1,output.shape[2]))\n",
        "    \n",
        "    #x=tf.reshape(x,[x.shape[0],x.shape[2]])\n",
        "    #print(self.dense3(x).shape)\n",
        "    #x = self.tanh(self.dense2(output)+self.dense3(x)+self.dense4(xu_emb)+\n",
        "                               #self.dense5(xi_emb)+self.dense6(emb_r))\n",
        "    #print(x.shape)\n",
        "    x = self.dense1(output)\n",
        "    #x = self.dense1(output)\n",
        "\n",
        "    return x,state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP3MZNqRRXTx"
      },
      "source": [
        "nrtrating = NRTrating()\n",
        "nrtdecoder = NRTdecoder()\n",
        "#nrtrating(tf.random.uniform((200,)),tf.random.uniform((200,)),tf.random.uniform((200,6)))\n",
        "#nrtdecoder(tf.random.uniform((200,1)),tf.random.uniform((200,20)),tf.random.uniform((200,40)),tf.random.uniform((200,40)),tf.random.uniform((200,4)))[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blVF2Pzue6S"
      },
      "source": [
        "def beam_decode(result_paths, fname, batch_raw, modules, consts, options):\n",
        "    \n",
        "    print (fname)\n",
        "\n",
        "    [SUMM_PATH_IDX, SUMM_PATH_WORD, MODEL_PATH_IDX, MODEL_PATH_WORD, RATING_PATH] = result_paths\n",
        "    beam_size = consts[\"beam_size\"]\n",
        "    num_live = 1\n",
        "    num_dead = 0\n",
        "    samples = []\n",
        "    sample_scores = np.zeros(beam_size)\n",
        "\n",
        "    last_traces = [[]]\n",
        "    last_scores = np.zeros(1)\n",
        "    last_states = []\n",
        "\n",
        "    XU, XI, Y_r, Y_r_vec, \\\n",
        "            Dict_lvt, Y_rev_tf, Y_rev_mark, \\\n",
        "            Y_sum_idx, Y_sum_mark, Y_sum_lvt, lvt_i2i = get_batch_data(batch_raw, consts, options, modules)\n",
        "    \n",
        "\n",
        "    rat_pred, dec_state, emb_r, xu_emb, xi_emb = nrtrating(XU, XI, consts[\"test_batch_size\"])\n",
        "    next_y = -np.ones((1, num_live, 1), dtype=\"int64\")\n",
        "    \n",
        "    write_rating(\"\".join((RATING_PATH, \"rating.\", fname)), str(rat_pred[0]) + \"\\t\" + str(Y_r[0]))\n",
        "\n",
        "    for step in range(consts[\"max_len_predict\"]):\n",
        "        model.decode_once\n",
        "        tile_emb_r = np.tile(emb_r, (num_live, 1))\n",
        "        tile_xu_emb = np.tile(xu_emb, (num_live, 1))\n",
        "        tile_xi_emb = np.tile(xi_emb, (num_live, 1))\n",
        "\n",
        "        y_pred, dec_state = nrtdecoder(next_y, dec_state, tile_emb_r, tile_xu_emb, tile_xi_emb, num_live, Dict_lvt)\n",
        "        \n",
        "        dict_size = y_pred.shape[-1]\n",
        "        cand_scores = last_scores + np.log(y_pred) # 分数最大越好\n",
        "        cand_scores = cand_scores.flatten()\n",
        "        idx_top_joint_scores = np.argsort(cand_scores)[-(beam_size - num_dead):]\n",
        "        idx_last_traces = idx_top_joint_scores // dict_size\n",
        "        idx_word_now = idx_top_joint_scores % dict_size\n",
        "        top_joint_scores = cand_scores[idx_top_joint_scores]\n",
        "\n",
        "        traces_now = []\n",
        "        scores_now = np.zeros((beam_size - num_dead))\n",
        "        states_now = []\n",
        "        for i, [j, k] in enumerate(zip(idx_last_traces, idx_word_now)):\n",
        "            traces_now.append(last_traces[j] + [lvt_i2i[k]])\n",
        "            scores_now[i] = copy.copy(top_joint_scores[i])\n",
        "            states_now.append(copy.copy(dec_state[j, :]))\n",
        "\n",
        "        num_live = 0\n",
        "        last_traces = []\n",
        "        last_scores = []\n",
        "        last_states = []\n",
        "\n",
        "        for i in range(len(traces_now)):\n",
        "            if traces_now[i][-1] == modules[\"eos_emb\"]:\n",
        "                samples.append([str(e) for e in traces_now[i][:-1]])\n",
        "                sample_scores[num_dead] = scores_now[i]\n",
        "                num_dead += 1\n",
        "            else:\n",
        "                last_traces.append(traces_now[i])\n",
        "                last_scores.append(scores_now[i])\n",
        "                last_states.append(states_now[i])\n",
        "                num_live += 1\n",
        "        if num_live == 0 or num_dead >= beam_size:\n",
        "            break\n",
        "\n",
        "        last_scores = np.array(last_scores).reshape((num_live, 1))\n",
        "        next_y = np.array([e[-1] for e in last_traces], dtype = \"int64\").reshape((1, num_live, 1))\n",
        "        dec_state = np.array(last_states).reshape((num_live, dec_state.shape[-1]))\n",
        "        assert num_live + num_dead == beam_size\n",
        "\n",
        "    if num_live > 0:\n",
        "        for i in range(num_live):\n",
        "            samples.append([str(e) for e in last_traces[i]])\n",
        "            sample_scores[num_dead] = last_scores[i]\n",
        "            num_dead += 1\n",
        "    \n",
        "    \n",
        "    #weight by length\n",
        "    for i in range(len(sample_scores)):\n",
        "        sent_len = float(len(samples[i]))\n",
        "        #sample_scores[i] = sample_scores[i] * math.exp(-sent_len / 8)\n",
        "        # Google's Neural Machine Translation System\n",
        "        lpy = math.pow((consts[\"min_len_predict\"] + sent_len), 0.6) / math.pow((consts[\"min_len_predict\"] + 1), 0.6)\n",
        "        sample_scores[i] = sample_scores[i]  / lpy\n",
        "\n",
        "    idx_sorted_scores = np.argsort(sample_scores) # 低分到高分\n",
        "    ly = int(Y_sum_mark[:, 0].sum())\n",
        "    y_true = Y_sum_idx[0 : ly, 0, 0].tolist()\n",
        "    y_true = [str(i) for i in y_true[:-1]] # delete <eos>\n",
        "\n",
        "    sorted_samples = []\n",
        "    sorted_scores = []\n",
        "    filter_idx = []\n",
        "    for e in idx_sorted_scores:\n",
        "        if len(samples[e]) >= consts[\"min_len_predict\"] and (str(modules[\"unk_emb\"]) not in set(samples[e])):\n",
        "            filter_idx.append(e)\n",
        "    if len(filter_idx) == 0:\n",
        "        filter_idx = idx_sorted_scores\n",
        "    for e in filter_idx:\n",
        "        sorted_samples.append(samples[e])\n",
        "        sorted_scores.append(sample_scores[e])\n",
        "\n",
        "    num_samples = len(sorted_samples)\n",
        "    if len(sorted_samples) == 1:\n",
        "        sorted_samples = sorted_samples[0]\n",
        "        num_samples = 1\n",
        "    \n",
        "    try:\n",
        "        if beam_size == 1:\n",
        "            write_summ(\"\".join((SUMM_PATH_IDX, \"summ.\", fname)), sorted_samples, 1)\n",
        "        else:\n",
        "            write_summ(\"\".join((SUMM_PATH_IDX, \"summ.\", fname)), sorted_samples[-1], 1)\n",
        "\t    \n",
        "        write_summ(\"\".join((MODEL_PATH_IDX, \"model.\", fname)), y_true, 1)\n",
        "\n",
        "        write_summ(\"\".join((SUMM_PATH_WORD, \"summ.\", fname)), sorted_samples, num_samples, modules[\"i2w\"], sorted_scores)\n",
        "\t    \n",
        "        write_summ(\"\".join((MODEL_PATH_WORD, \"model.\", fname)), y_true, 1, modules[\"i2w\"])\n",
        "    except Exception as e:\n",
        "        print (\"error\", fname)\n",
        "        raise\n",
        "\n",
        "\n",
        "def predict(data_type, model, modules, consts, options):\n",
        "    predict_rmse(data_type,  modules, consts, options)\n",
        "    predict_summary(data_type,  modules, consts, options)\n",
        "\n",
        "\n",
        "def predict_summary(data_type, model, modules, consts, options):\n",
        "    print (\"start predicting summary ...\")\n",
        "    SUMM_PATH_IDX = SUMM_PATH + data_type + \"/sum_idx/\"\n",
        "    SUMM_PATH_WORD = SUMM_PATH + data_type + \"/sum_word/\"\n",
        "    MODEL_PATH_IDX = SUMM_PATH + data_type + \"/model_idx/\"\n",
        "    MODEL_PATH_WORD = SUMM_PATH + data_type + \"/model_word/\"\n",
        "    RATING_PATH = SUMM_PATH + data_type + \"/rating/\"\n",
        "    \n",
        "    if not exists(SUMM_PATH_IDX):\n",
        "        makedirs(SUMM_PATH_IDX)\n",
        "    if not exists(SUMM_PATH_WORD):\n",
        "        makedirs(SUMM_PATH_WORD)\n",
        "    if not exists(MODEL_PATH_IDX):\n",
        "        makedirs(MODEL_PATH_IDX)\n",
        "    if not exists(MODEL_PATH_WORD):\n",
        "        makedirs(MODEL_PATH_WORD)\n",
        "    if not exists(RATING_PATH):\n",
        "        makedirs(RATING_PATH)\n",
        "    \n",
        "    rebuild_dir(SUMM_PATH_IDX)\n",
        "    rebuild_dir(SUMM_PATH_WORD)\n",
        "    rebuild_dir(MODEL_PATH_IDX)\n",
        "    rebuild_dir(MODEL_PATH_WORD)\n",
        "    rebuild_dir(RATING_PATH)\n",
        "\n",
        "    result_paths = [SUMM_PATH_IDX, SUMM_PATH_WORD, MODEL_PATH_IDX, MODEL_PATH_WORD, RATING_PATH]\n",
        "\n",
        "\n",
        "    [x_raw_train, x_raw_dev, x_raw_test, old_dic, dic, hfw, i2w, w2i, w2w, w2idf, i2user, user2i, i2item, item2i, user2w, item2w] = modules[\"all_data\"]\n",
        "    batch_list = datar.get_batch_index(len(x_raw_dev), consts[\"test_batch_size\"], options[\"is_predicting\"])\n",
        "    error_i = 0\n",
        "    for batch_index in batch_list:\n",
        "        local_batch_size = len(batch_index)\n",
        "        batch_raw = [x_raw_dev[bxi] for bxi in batch_index]\n",
        "        beam_decode(result_paths, str(batch_index[0]), batch_raw, modules, consts, options)\n",
        "        error_i += 1\n",
        "        if error_i == consts[\"test_samples\"]:\n",
        "            break\n",
        "\n",
        "def predict_rmse(data_type, modules, consts, options):\n",
        "    print (\"start predicting rmse...\")\n",
        "\n",
        "    [x_raw_train, x_raw_dev, x_raw_test, old_dic, dic, hfw, i2w, w2i, w2w, w2idf, i2user, user2i, i2item, item2i, user2w, item2w] = modules[\"all_data\"]\n",
        "    batch_list = datar.get_batch_index(len(x_raw_dev), consts[\"batch_size\"])\n",
        "   \n",
        "    ae = 0\n",
        "    se = 0\n",
        "    se_i = 0\n",
        "    \n",
        "    for batch_index in batch_list:\n",
        "        local_batch_size = len(batch_index)\n",
        "        batch_raw = [x_raw_dev[bxi] for bxi in batch_index]\n",
        "        XU, XI, Y_r, Y_r_vec, \\\n",
        "            Dict_lvt, Y_rev_tf, Y_rev_mark, \\\n",
        "            Y_sum_idx, Y_sum_mark, Y_sum_lvt, lvt_i2i = get_batch_data(batch_raw, consts, options, modules)\n",
        "\n",
        "        rat_pred, dec_state, emb_r, xu_emb, xi_emb = nrtrating(XU, XI, Y_r_vec)\n",
        "\n",
        "        for i in range(len(batch_raw)):\n",
        "            se += (rat_pred[i, 0] -  Y_r[i]) * (rat_pred[i, 0] -  Y_r[i])\n",
        "            ae += abs(rat_pred[i, 0] -  Y_r[i])\n",
        "        se_i += len(batch_raw)\n",
        "        \n",
        "        #print \"MAE now = \", ae / se_i,\n",
        "        #print \"RMSE now = \", np.sqrt(se / se_i), \n",
        "        #print \"#X = \", se_i\n",
        "\n",
        "    print (\"MAE = \", ae / se_i,)\n",
        "    print (\"RMSE = \", np.sqrt(se / se_i), )\n",
        "    print (\"#X = \", se_i)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "source": [
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def cost_mse(pred,label):\n",
        "  return tf.reduce_sum(tf.keras.losses.mean_squared_error(label,pred))\n",
        "def cost_mae(pred,label):\n",
        "  return tf.reduce_sum(tf.keras.losses.mean_absolute_error(label,pred))\n",
        "\n",
        "def cost_nll(pred,label,mark):\n",
        "  cost = -tf.math.log(pred)*label*mark\n",
        "  cost = tf.reduce_sum(tf.reduce_sum(cost,axis=1))\n",
        "  return cost\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=nrtrating,\n",
        "                                 decoder=nrtdecoder)\n",
        "\n",
        "@tf.function\n",
        "def train_step(xu, xi, y_rating, y_rat_vec, lvt_dict, y_rev_mark, y_rev_tf, y_sum_idx, y_sum_mark, y_sum_lvt,batch_size,lr):\n",
        "  loss = 0\n",
        "  cce = 0\n",
        "  mse = 0\n",
        "  nll = 0\n",
        "  nllu = 0\n",
        "  nlli = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    rating,review,review_i,review_u,x_emb,i_emb,hru3,hri3,hrui3,dec_hidden = nrtrating(xu,xi,y_rat_vec)\n",
        "    \n",
        "    dec_input = tf.expand_dims([modules['<start>']] *y_sum_idx.shape[1] , 1)\n",
        "    y_sum_idx = tf.transpose(tf.reshape(y_sum_idx,[y_sum_idx.shape[0],y_sum_idx.shape[1]]))\n",
        "    y_sum_mark = tf.transpose(tf.reshape(y_sum_mark,[y_sum_mark.shape[0],y_sum_mark.shape[1]]))\n",
        "    print(y_sum_idx.shape,dec_hidden.shape)\n",
        "    ce = 0\n",
        "    for t in range(1,y_sum_idx.shape[1]):\n",
        "      #print(dec_hidden.shape,dec_input.shape)\n",
        "      predictions,dec_hidden = nrtdecoder(dec_input,dec_hidden,x_emb,i_emb,y_rat_vec)\n",
        "      #predictions, dec_hidden = nrtdecoder(dec_input,dec_hidden)\n",
        "      ce+= loss_function(y_sum_idx[:,t],predictions)\n",
        "\n",
        "      #using teacher forcing\n",
        "      dec_input = tf.expand_dims(y_sum_idx[:,t],1)\n",
        "    #y_sum_pred = nrtdecoder(x_emb,i_emb,emb_r,hru3,hri3,hrui3)\n",
        "    cce += ce/ int(xu.shape[0])\n",
        "    mse+= cost_mse(rating,y_rating) / int(xu.shape[0])\n",
        "    nll+= cost_nll(review,y_rev_tf,y_rev_mark) / int(xu.shape[0])\n",
        "    nllu+= cost_nll(review_u,y_rev_tf,y_rev_mark) / int(xu.shape[0])\n",
        "    nlli += cost_nll(review_i,y_rev_tf,y_rev_mark)/ int(xu.shape[0])\n",
        "    loss += cce + mse+nll+nllu+nlli\n",
        "  batch_loss = (loss)\n",
        "\n",
        "  variables = nrtrating.trainable_variables + nrtdecoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return [batch_loss,cce,mse,nll,nllu,nlli],rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_vKI0FP00wR"
      },
      "source": [
        "def run(data_type, existing_model_name = None):\n",
        "    modules, consts, options = init_modules(data_type)\n",
        "    \n",
        "    if options[\"is_predicting\"]:\n",
        "        need_load_model = True\n",
        "        training_model = False\n",
        "    else:\n",
        "        need_load_model = False\n",
        "        training_model = True\n",
        "\n",
        "\n",
        "    print (\"compiling...\")\n",
        "    #model = NeuralRecsys(modules, consts, options)\n",
        "    \n",
        "\n",
        "    if training_model:\n",
        "        print (\"training......\")\n",
        "        [x_raw_train, x_raw_dev, x_raw_test, old_dic, dic, hfw, i2w, w2i, w2w, w2idf, i2user, user2i, i2item, item2i, user2w, item2w] = modules[\"all_data\"]\n",
        "        for epoch in range(0, 50):\n",
        "            \n",
        "            if epoch > 2:\n",
        "                consts[\"lr\"] = 0.1\n",
        "\n",
        "            start = time.time()\n",
        "            batch_list = get_batch_index(len(x_raw_train), consts[\"batch_size\"]) #consts[\"batch_size\"]\n",
        "            error = 0\n",
        "            error_i = 0\n",
        "            error_rmse = 0\n",
        "            e_nll = 0\n",
        "            e_nllu = 0\n",
        "            e_nlli = 0\n",
        "            e_cce = 0\n",
        "            e_mae = 0\n",
        "            e_l2n = 0\n",
        "            for batch_index in batch_list:\n",
        "                local_batch_size = len(batch_index)\n",
        "                batch_raw = [x_raw_train[bxi] for bxi in batch_index]\n",
        "                load_time = time.time()\n",
        "                XU, XI, Y_r, Y_r_vec, \\\n",
        "                        Dict_lvt, Y_rev_tf, Y_rev_mark, \\\n",
        "                        Y_sum_idx, Y_sum_mark, Y_sum_lvt, lvt_i2i = get_batch_data(batch_raw, consts, options, modules)\n",
        "                #print(type(XU))\n",
        "                #ypad = tf.pad(Y_sum_idx,tf.constant([[1,0],[0,0],[0,0]]),\"constant\")\n",
        "                #yupdate = tf.slice(ypad,[0,0,0],[21,-1,-1])\n",
        "                #Y_sum_idx = yupdate\n",
        "                #print(Y_sum_idx,Y_sum_mark)\n",
        "                \n",
        "                cost,rating = train_step(XU, XI, Y_r, Y_r_vec, \\\n",
        "                                                   Dict_lvt, Y_rev_tf, Y_rev_mark, \\\n",
        "                                                   Y_sum_idx, Y_sum_mark, Y_sum_lvt, \\\n",
        "                                                   local_batch_size, consts[\"lr\"])\n",
        "                #print cost, mse, nll, cce, l2n\n",
        "                error += cost[0]\n",
        "                e_cce += cost[1]\n",
        "                error_rmse+= cost[2]\n",
        "                e_nll += cost[3]\n",
        "                e_nllu += cost[4]\n",
        "                e_nlli += cost[5]\n",
        "\n",
        "\n",
        "                #if(error_i==19):\n",
        "                  #print(Y_r,tf.convert_to_tensor(rating))\n",
        "                error_i+=1  \n",
        "                if error_i % 50 == 0:\n",
        "                  print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,error_i,cost[0].numpy()))\n",
        "            # saving (checkpoint) the model every 2 epochs\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "              checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "            #print_sent_dec(sum_pred, Y_sum_idx, Y_sum_mark, modules, consts, options, lvt_i2i)\n",
        "            print (\"epoch=\", epoch, \", Error = \", error / len(batch_list),)\n",
        "            print (\", RMSE = \", error_rmse / len(batch_list), \", time=\", time.time()-start)\n",
        "            print (\"epoch=\", epoch, \", Error = \", error / len(batch_list),\",cce = \",e_cce/len(batch_list),\", mse = \",error_rmse/len(batch_list),\", nll =\",e_nll/len(batch_list),\n",
        "                   \", nllu= \",e_nllu/len(batch_list), \", nlli =\", e_nlli/len(batch_list))\n",
        "            #print (\", RMSE = \", error_rmse / len(batch_list), \", time=\", time.time()-start)\n",
        "    else:\n",
        "      # restoring the latest checkpoint in checkpoint_dir\n",
        "      checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "      predict(data_type, modules, consts, options)\n",
        "run('amazon')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOE5Waamxdq9"
      },
      "source": [
        "run('amazon',None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5GWJFFgxuGH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
