# -*- coding: utf-8 -*-
"""Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17CXoiw_mEbuSrP_wG-x-0m-893_2kcQd
"""



"""# Original T5 Summarization

"""

# data=
# Uncomment this and add the list of data to be summarized here
import torch
import json
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
import time
device ='cuda'
model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)
tokenizer = T5Tokenizer.from_pretrained('t5-base')
summaries_org=[]
lengths=[]
times = []
t0=time.time()
for x in range(data):
    
    original = x
    lengths.append(len(data))
    t5_prepared_Text="summarize: "+original
    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors="pt").to(device)
    summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=10,
                                    max_length=20,
                                    early_stopping=True)
    
    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    times.append(time.time()-t0)
    summaries_org.append(output)
    print('Generated summary', x)
    t0=time.time()
    print(summaries_org)

def split_512_tokens(data, length_to_split):
    l_total=[]
    l_parcial = []
    if len(data.split())//500 > 0:
        n = len(data.split())//500
    else:
        n=1
    for w in range(n):
        if w==0:
            l_parcial = data.split()[:500]
            l_total.append(" ".join(l_parcial))
        else:
            l_parcial = data.split()[w*450:w*450 + 500]
            l_total.append(" ".join(l_parcial))
    return l_total
 
 
 
 

device ='cuda'
model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)
tokenizer = T5Tokenizer.from_pretrained('t5-base')
summaries=[]
lengths=[]
times = []
t0=time.time()
for x in range(data):
    original = x 
    lengths.append(len(original))
    splitted = split_512_tokens(original, 200)
    summary_chunks=""
    for y in splitted:
        t5_prepared_Text="summarize: "+y
        tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors="pt").to(device)
        summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=20,
                                    max_length=100,
                                    early_stopping=True)
    
        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        summary_chunks+=" "+output
    t5_prepared_Text="summarize: "+summary_chunks
    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors="pt").to(device)
    summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=20,
                                    max_length=100,
                                    early_stopping=True)
    
    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
    
    times.append(time.time()-t0)
    summaries.append(output)
    print('Generated summary', x)
    t0=time.time()
    print(summaries)