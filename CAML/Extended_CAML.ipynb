{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of CAML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-BAaPo25Nzp"
      },
      "source": [
        "#Import Stmts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swMcFq9vCwad",
        "outputId": "e6e537ea-8e3d-4f3f-c46f-8d55f4e332ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x5vYK2YKRM-"
      },
      "source": [
        "\r\n",
        "from collections import Counter\r\n",
        "import csv\r\n",
        "import argparse\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from datetime import datetime\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import codecs\r\n",
        "np.random.seed(1337)  # for reproducibility\r\n",
        "random.seed(1337)\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import sys\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from collections import Counter\r\n",
        "#import cPickle as pickle\r\n",
        "import pickle\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "import string\r\n",
        "import re\r\n",
        "import math\r\n",
        "import operator\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "import sys\r\n",
        "from nltk.corpus import stopwords\r\n",
        "#from nltk.translate.bleu_score import corpus_bleu\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from scipy.stats import pearsonr\r\n",
        "from scipy.stats import spearmanr\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from collections import defaultdict\r\n",
        "from collections import OrderedDict\r\n",
        "#from Rouge155_modify import Rouge155\r\n",
        "import sys\r\n",
        "\r\n",
        "PAD = \"<PAD>\"\r\n",
        "UNK = \"<UNK>\"\r\n",
        "SOS = \"<SOS>\"\r\n",
        "EOS = \"<EOS>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9y7K2HtU63p"
      },
      "source": [
        "#Util functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2oIa68BKpFr"
      },
      "source": [
        "\r\n",
        "def batchify(data, i, bsz, max_sample):\r\n",
        "    start = int(i * bsz)\r\n",
        "    end = int(i * bsz) + bsz\r\n",
        "    if(end>max_sample):\r\n",
        "        end = max_sample\r\n",
        "    return data[start:end]\r\n",
        "\r\n",
        "def mkdir_p(path):\r\n",
        "    ''' Makes path if path does not exist\r\n",
        "    '''\r\n",
        "    if path == '':\r\n",
        "        return\r\n",
        "    try:\r\n",
        "        os.makedirs(path)\r\n",
        "    except OSError as exc:  # Python >2.5\r\n",
        "        pass\r\n",
        "def print_args(args, path=None):\r\n",
        "    ''' Print arguments to log file\r\n",
        "    '''\r\n",
        "    if path:\r\n",
        "        output_file = open(path, 'w')\r\n",
        "    args.command = ' '.join(sys.argv)\r\n",
        "    items = vars(args)\r\n",
        "    output_file.write('=============================================== \\n')\r\n",
        "    for key in sorted(items.keys(), key=lambda s: s.lower()):\r\n",
        "        value = items[key]\r\n",
        "        if not value:\r\n",
        "            value = \"None\"\r\n",
        "        if path is not None:\r\n",
        "            output_file.write(\"  \" + key + \": \" + str(items[key]) + \"\\n\")\r\n",
        "    output_file.write('=============================================== \\n')\r\n",
        "    if path:\r\n",
        "        output_file.close()\r\n",
        "    del args.command\r\n",
        "\r\n",
        "def show_stats(name, x):\r\n",
        "    print(\"{} max={} mean={} min={}\".format(name,\r\n",
        "                                        np.max(x),\r\n",
        "                                        np.mean(x),\r\n",
        "                                        np.min(x)))\r\n",
        "    \r\n",
        "def prep_data_list(data, max_length): \r\n",
        "\r\n",
        "    all_data = []\r\n",
        "    all_lengths = []\r\n",
        "    for doc in tqdm(data, desc='building List'):\r\n",
        "        sent_lens = len(doc)\r\n",
        "        if sent_lens > max_length:\r\n",
        "            sent_lens = max_length\r\n",
        "        \r\n",
        "        _data_list = pad_to_max(doc, max_length)\r\n",
        "        all_data.append(_data_list)\r\n",
        "        all_lengths.append(sent_lens)\r\n",
        "\r\n",
        "    return all_data, all_lengths\r\n",
        "\r\n",
        "def pad_to_max(seq, seq_max, pad_token=0):\r\n",
        "    ''' Pad Sequence to sequence max\r\n",
        "    '''\r\n",
        "    new_seq = seq[:]\r\n",
        "    while(len(new_seq)<seq_max):\r\n",
        "        new_seq.append(pad_token)\r\n",
        "    return new_seq[:seq_max]\r\n",
        "\r\n",
        "def prep_hierarchical_data_list_new(data, lens, smax, dmax, threshold=True):\r\n",
        "    \"\"\" Converts and pads hierarchical data\r\n",
        "    \"\"\"\r\n",
        "    # print(\"Preparing Hiearchical Data list\")\r\n",
        "\r\n",
        "    # print(data[0])\r\n",
        "    #print(data)\r\n",
        "\r\n",
        "    all_data = []\r\n",
        "    all_lengths = []\r\n",
        "    #for i in tqdm(range(len(data)), desc='building H-dict'):\r\n",
        "    for i in range(len(data)):\r\n",
        "        new_data = []\r\n",
        "        data_lengths = []\r\n",
        "        #for data_list in doc:\r\n",
        "        doc = data[i]\r\n",
        "        doc_len = lens[i]\r\n",
        "        #print (doc_len)\r\n",
        "        for j in range(len(doc)):\r\n",
        "            data_list = doc[j]\r\n",
        "            sent_lens = doc_len[j]\r\n",
        "            # for each document\r\n",
        "            #sent_lens = len(data_list)\r\n",
        "            #print (sent_lens)\r\n",
        "            if(sent_lens==0):\r\n",
        "                continue\r\n",
        "            if(threshold and sent_lens>smax):\r\n",
        "                sent_lens = smax\r\n",
        "\r\n",
        "            _data_list = pad_to_max(data_list, smax)\r\n",
        "            new_data.append(_data_list)\r\n",
        "            data_lengths.append(sent_lens)\r\n",
        "        new_data = pad_to_max(new_data, dmax,\r\n",
        "                            pad_token=[0 for i in range(smax)])\r\n",
        "\r\n",
        "        _new_data = []\r\n",
        "        for nd in new_data:\r\n",
        "            # flatten lists\r\n",
        "            _new_data += nd\r\n",
        "\r\n",
        "        data_lengths = pad_to_max(data_lengths, dmax, pad_token=0)\r\n",
        "        all_data.append(_new_data)\r\n",
        "        all_lengths.append(data_lengths)\r\n",
        "    return all_data, all_lengths\r\n",
        "\r\n",
        "def hierarchical_flatten(embed, lengths, smax):\r\n",
        "    \"\"\" Flattens embedding for hierarchical processing.\r\n",
        "    Args:\r\n",
        "        embed: `tensor` [bsz x (num_docs * seq_len) x dim]\r\n",
        "        lengths: `tensor` [bsz x num_docs]\r\n",
        "        smax: `int` - maximum number of words in sentence\r\n",
        "    Returns:\r\n",
        "        embed: `tensor` [bsz x seq_len x dim] flattened input\r\n",
        "        lengths: `tensor` [bsz] flattend lengths\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _dims = embed.get_shape().as_list()[2]\r\n",
        "    embed = tf.reshape(embed, [-1, smax, _dims])\r\n",
        "    lengths = tf.reshape(lengths, [-1])\r\n",
        "    return embed, lengths\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3--bnPU__Q"
      },
      "source": [
        "#Parent experiment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI-Bz_nfNyjE"
      },
      "source": [
        "\r\n",
        "class Experiment(object):\r\n",
        "    ''' Implements a base experiment class for TensorFLow\r\n",
        "    Contains commonly used util functions.\r\n",
        "    Extend this base Experiment class\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.uuid = datetime.now().strftime(\"%d_%H:%M:%S\")\r\n",
        "        self.eval_test = defaultdict(list)\r\n",
        "        self.eval_train = defaultdict(list)\r\n",
        "        self.eval_dev = defaultdict(list)\r\n",
        "        self.eval_test2 = defaultdict(list)\r\n",
        "        self.eval_dev2 = defaultdict(list)\r\n",
        "        self.wiggle = False\r\n",
        "        self.loggers = defaultdict(dict)\r\n",
        "\r\n",
        "    def register_to_log(self, set_type, epoch, attr, val):\r\n",
        "        if(attr not in self.loggers):\r\n",
        "            self.loggers[attr] = {'train':defaultdict(dict),\r\n",
        "                                    'Dev':defaultdict(dict),\r\n",
        "                                    'Test':defaultdict(dict)}\r\n",
        "        self.loggers[attr][set_type][epoch] = val\r\n",
        "\r\n",
        "    def dump_all_logs(self):\r\n",
        "        for key, value in self.loggers.items():\r\n",
        "            for set_type, data in value.items():\r\n",
        "                self.write_log_values(data, key, set_type)\r\n",
        "\r\n",
        "    def write_log_values(self, data, attr, set_type):\r\n",
        "        fp = self.out_dir +'./{}_{}.log'.format(attr, set_type)\r\n",
        "        with open(fp, 'w+') as f:\r\n",
        "            writer = csv.writer(f, delimiter='\\t')\r\n",
        "            writer.writerows(data.items())\r\n",
        "\r\n",
        "    def _build_char_index(self):\r\n",
        "        all_chars = list(string.printable)\r\n",
        "        self.char_index = {char:index+2 for index, char in enumerate(all_chars)}\r\n",
        "        self.char_index['<pad>'] = 0\r\n",
        "        self.char_index['<unk>'] = 1\r\n",
        "\r\n",
        "    def _setup(self):\r\n",
        "        ''' Full Setup Procedure\r\n",
        "        '''\r\n",
        "         # Make directory for log file and saving models\r\n",
        "        self._make_dir()\r\n",
        "        # Select GPU\r\n",
        "        self._designate_gpu()\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    def _designate_gpu(self):\r\n",
        "        ''' Choose GPU to use\r\n",
        "        '''\r\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n",
        "        if(self.args.gpu == '-1'):\r\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n",
        "        else:\r\n",
        "            print(\"Selecting GPU no.{}\".format(self.args.gpu))\r\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self.args.gpu)\r\n",
        "\r\n",
        "    def _make_dir(self):\r\n",
        "        ''' Make log directories\r\n",
        "        '''\r\n",
        "        # self.model_name = self.args.rnn_type\r\n",
        "        self.hyp_str = self.uuid + '_' + self.model_name\r\n",
        "        if(self.args.log == 1):\r\n",
        "            self.out_dir = './{}/{}/{}/{}/'.format(\r\n",
        "                self.args.log_dir,\r\n",
        "                self.args.dataset, self.model_name, self.uuid)\r\n",
        "            # print(self.out_dir)\r\n",
        "            mkdir_p(self.out_dir)\r\n",
        "            self.mdl_path = self.out_dir + '/mdl.ckpt'\r\n",
        "            self.path = self.out_dir + '/logs.txt'\r\n",
        "            print_args(self.args, path=self.path)\r\n",
        "\r\n",
        "    def write_to_file(self, txt):\r\n",
        "        ''' A wrapper for printing to log and on CLI\r\n",
        "        '''\r\n",
        "        try:\r\n",
        "            if(self.args.log == 1):\r\n",
        "                with open(self.path, 'a+') as f:\r\n",
        "                    f.write(txt + '\\n')\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "        print(txt)\r\n",
        "\r\n",
        "    \r\n",
        "            \r\n",
        "    \r\n",
        "    def _register_eval_score(self, epoch, eval_type, metric, val):\r\n",
        "        \"\"\" Registers eval metrics to class\r\n",
        "        \"\"\"\r\n",
        "        eval_obj = {\r\n",
        "            'metric':metric,\r\n",
        "            'val':val\r\n",
        "        }\r\n",
        "\r\n",
        "        if(eval_type.lower()=='dev'):\r\n",
        "            self.eval_dev[epoch].append(eval_obj)\r\n",
        "        elif(eval_type.lower()=='test'):\r\n",
        "            self.eval_test[epoch].append(eval_obj)\r\n",
        "        elif(eval_type.lower()=='train'):\r\n",
        "            self.eval_train[epoch].append(eval_obj)\r\n",
        "        elif(eval_type.lower()=='dev2'):\r\n",
        "            self.eval_dev2[epoch].append(eval_obj)\r\n",
        "        elif(eval_type.lower()=='test2'):\r\n",
        "            self.eval_test2[epoch].append(eval_obj)\r\n",
        "\r\n",
        "    def _show_metrics(self, epoch, eval_list, show_metrics, name):\r\n",
        "        \"\"\" Shows and outputs metrics\r\n",
        "        \"\"\"\r\n",
        "        # print(\"Eval Metrics for [{}]\".format(name))\r\n",
        "        get_last = eval_list[epoch]\r\n",
        "        for metric in get_last:\r\n",
        "            # print(metric)\r\n",
        "            if(metric['metric'] in show_metrics):\r\n",
        "                self.write_to_file(\"[{}] {}={}\".format(name,\r\n",
        "                                                    metric['metric'],\r\n",
        "                                                    metric['val']))\r\n",
        "\r\n",
        "\r\n",
        "    def _select_test_by_dev(self, epoch, eval_dev, eval_test,\r\n",
        "                            no_test=False, lower_is_better=False,\r\n",
        "                            name='', has_dev=True):\r\n",
        "        \"\"\" Outputs best test score based on dev score\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.write_to_file(\"====================================\")\r\n",
        "        primary_metrics = []\r\n",
        "        test_metrics = []\r\n",
        "        if(lower_is_better):\r\n",
        "            reverse=False\r\n",
        "        else:\r\n",
        "            reverse=True\r\n",
        "        # print(eval_dev)\r\n",
        "\r\n",
        "        if(has_dev==True):\r\n",
        "            for key, value in eval_dev.items():\r\n",
        "                _val = [x for x in value if x['metric']==self.eval_primary]\r\n",
        "                if(len(_val)==0):\r\n",
        "                    continue\r\n",
        "                primary_metrics.append([key, _val[0]['val']])\r\n",
        "\r\n",
        "            sorted_metrics = sorted(primary_metrics,\r\n",
        "                                        key=operator.itemgetter(1),\r\n",
        "                                        reverse=reverse)\r\n",
        "            cur_dev_score = primary_metrics[-1][1]\r\n",
        "            best_epoch = sorted_metrics[0][0]\r\n",
        "\r\n",
        "            if(no_test):\r\n",
        "                # For MNLI or no test set\r\n",
        "                print(\"[{}] Best epoch={}\".format(name, best_epoch))\r\n",
        "                self._show_metrics(best_epoch, eval_dev,\r\n",
        "                                    self.show_metrics, name='best')\r\n",
        "                if(self.args.wiggle_score>0 and self.wiggle==False):\r\n",
        "                    if(cur_dev_score>self.args.wiggle_score):\r\n",
        "                        print(\"Cur Dev Score at {}\".format(cur_dev_score))\r\n",
        "                        print(\"Activating Wiggle-SGD mode\")\r\n",
        "                        self.wiggle=True\r\n",
        "                return best_epoch, cur_dev_score\r\n",
        "        else:\r\n",
        "            best_epoch = -1\r\n",
        "\r\n",
        "        for key, value in eval_test.items():\r\n",
        "            _val = [x for x in value if x['metric']==self.eval_primary]\r\n",
        "            if(len(_val)==0):\r\n",
        "                continue\r\n",
        "            test_metrics.append([key, _val[0]['val']])\r\n",
        "\r\n",
        "        # if(len(primary_metrics)==0):\r\n",
        "        #     return False\r\n",
        "\r\n",
        "        sorted_test = sorted(test_metrics, key=operator.itemgetter(1),\r\n",
        "                                    reverse=reverse)\r\n",
        "\r\n",
        "        max_epoch = sorted_test[0][0]\r\n",
        "\r\n",
        "        self.write_to_file(\"Best epoch={}\".format(best_epoch))\r\n",
        "        self._show_metrics(best_epoch, eval_test,\r\n",
        "                            self.show_metrics, name='best')\r\n",
        "        self.write_to_file(\"Maxed epoch={}\".format(max_epoch))\r\n",
        "        self._show_metrics(max_epoch, eval_test,\r\n",
        "                            self.show_metrics, name='max')\r\n",
        "        if(self.args.early_stop>0):\r\n",
        "            # Use early stopping\r\n",
        "            if(epoch - best_epoch > self.args.early_stop):\r\n",
        "                # print(\"Ended at early stop..\")\r\n",
        "                return True, max_epoch, best_epoch\r\n",
        "        if(self.args.wiggle_after>0 and self.wiggle==False):\r\n",
        "            # use SGD wiggling\r\n",
        "            if(epoch - best_epoch > self.args.wiggle_after):\r\n",
        "                print(\"Activating Wiggle-SGD mode\")\r\n",
        "                self.wiggle = True\r\n",
        "        if(self.args.wiggle_score>0 and self.wiggle==False):\r\n",
        "            if(cur_dev_score>self.args.wiggle_score):\r\n",
        "                print(\"Cur Dev Score at {}\".format(cur_dev_score))\r\n",
        "                print(\"Activating Wiggle-SGD mode\")\r\n",
        "                self.wiggle=True\r\n",
        "\r\n",
        "        return False, max_epoch, best_epoch\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJSJoQUVcXW"
      },
      "source": [
        "#Args defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNo6NrCPS2ej"
      },
      "source": [
        "\r\n",
        "class args():\r\n",
        "  def __init__(self):\r\n",
        "    self.smax = 30\r\n",
        "    self.dmax = 20\r\n",
        "    self.base_encoder = 'NBOW'\r\n",
        "    self.rnn_type = 'RAW_MSE_CAML_FN_FM'\r\n",
        "    self.data_link = '/content/drive/MyDrive/CAML_TOY_DATA/movie100'\r\n",
        "    self.log_dir = 'logs'\r\n",
        "    self.log =1 \r\n",
        "    self.dataset ='Amazon_Electronics'\r\n",
        "    self.gpu = 0\r\n",
        "    self.dev = 1\r\n",
        "    self.init = 0.01\r\n",
        "    self.gmax = 30\r\n",
        "    self.epochs = 50 #\r\n",
        "    self.batch_size = 50 #\r\n",
        "    self.implicit = 1\r\n",
        "    self.learn_rate = 1e-3\r\n",
        "    self.num_class = 6\r\n",
        "    self.dropout = 0.2\r\n",
        "    self.rnn_dropout = 0.2\r\n",
        "    self.emb_dropout = 0.2\r\n",
        "    self.dev_lr = 0\r\n",
        "    self.decay_steps = 0\r\n",
        "    self.decay_epoch = 0\r\n",
        "    self.emb_size = 50\r\n",
        "    self.latent_size=50\r\n",
        "    self.rnn_size = 50\r\n",
        "    self.num_heads = 2\r\n",
        "    self.data_prepare =1\r\n",
        "    self.key_word_lambda = 0.25\r\n",
        "    self.l2_reg = 1E-6\r\n",
        "    self.word_aggregate ='MAX'\r\n",
        "    self.self_num_heads = 2\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y_JI59OOauo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdGBs_6Olly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDAdN2_bM1O"
      },
      "source": [
        "#GumbelSoftmax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttcW8fG4Rq_4"
      },
      "source": [
        "\n",
        "class GumbelSoftmax(tf.keras.layers.Layer): #checkthis\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        \"\"\"Initialization method.\n",
        "        Args:\n",
        "            axis (int): Axis to perform the softmax operation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Overrides its parent class with any custom arguments if needed\n",
        "        super(GumbelSoftmax, self).__init__(**kwargs)\n",
        "\n",
        "        # Defining a property for holding the intended axis\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs, tau, hard=1):\n",
        "        \"\"\"Method that holds vital information whenever this class is called.\n",
        "        Args:\n",
        "            x (tf.Tensor): A tensorflow's tensor holding input data.\n",
        "            tau (float): Gumbel-Softmax temperature parameter.\n",
        "        Returns:\n",
        "            Gumbel-Softmax output and its argmax token.\n",
        "        \"\"\"\n",
        "\n",
        "        # Adds a sampled Gumbel distribution to the input\n",
        "        x = inputs + self.gumbel_distribution(inputs.shape)\n",
        "\n",
        "        # Applying the softmax over the Gumbel-based input\n",
        "        y = tf.nn.softmax(x / tau, self.axis)\n",
        "        \n",
        "        if hard ==1:\n",
        "          # Sampling an argmax token from the Gumbel-based input\n",
        "          y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 1, keepdims=True)),\n",
        "                         y.dtype)\n",
        "          y = tf.stop_gradient(y_hard -y) + y\n",
        "        return y\n",
        "\n",
        "    def gumbel_distribution(self,input_shape, eps=1e-20):\n",
        "        \"\"\"Samples a tensor from a Gumbel distribution.\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of tensor to be sampled.\n",
        "        Returns:\n",
        "            An input_shape tensor sampled from a Gumbel distribution.\n",
        "        \"\"\"\n",
        "\n",
        "        # Samples an uniform distribution based on the input shape\n",
        "        uniform_dist = tf.random.uniform(input_shape, 0, 1)\n",
        "\n",
        "        # Samples from the Gumbel distribution\n",
        "        gumbel_dist = -1 * tf.math.log(-1 * tf.math.log(uniform_dist + eps) + eps)\n",
        "\n",
        "        return gumbel_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFHU4dNLSv9M"
      },
      "source": [
        "#Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msAG3ukgSute"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "  mask = tf.cast(tf.math.equal(mask, 0), tf.float32)\n",
        "  mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6qjqVQbTOd"
      },
      "source": [
        "#Coattention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29iYccilOsBn"
      },
      "source": [
        "\n",
        "\n",
        "#@title\n",
        "class Coattention(tf.keras.layers.Layer):\n",
        "  def __init__(self,  input_size,args, dropout= None):\n",
        "    super(Coattention,self).__init__()\n",
        "    self.args = args\n",
        "    self.fc_user = tf.keras.layers.Dense(input_size, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.fc_item = tf.keras.layers.Dense(input_size, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.initializer = tf.keras.initializers.GlorotUniform()\n",
        "    self.weights_U = tf.Variable(self.initializer(shape=(input_size, input_size)))\n",
        "    self.gumbel_softmax = GumbelSoftmax()\n",
        "    if dropout is not None:\n",
        "      self.dropout_user = tf.keras.layers.Dropout(dropout)\n",
        "      self.dropout_item = tf.keras.layers.Dropout(dropout)\n",
        "    else:\n",
        "      self.dropout_user = None\n",
        "\n",
        "  def call(self,user_reviews, item_reviews, user_mask, item_mask, pooling='MAX', gumbel= True, temp=0.5, hard=1):\n",
        "    orig_user_reviews = user_reviews\n",
        "    orig_item_reviews = item_reviews\n",
        "    \n",
        "    a_len = user_reviews.shape[1]\n",
        "    b_len = item_reviews.shape[1]\n",
        "    input_dim = user_reviews.shape[2]\n",
        "    max_len = a_len\n",
        "    dim = input_dim\n",
        "\n",
        "    user_reviews= self.fc_user(user_reviews)\n",
        "    item_reviews = self.fc_item(item_reviews)\n",
        "\n",
        "    _a = tf.reshape(user_reviews, [-1, dim])\n",
        "    \n",
        "    z = tf.matmul(_a, self.weights_U)\n",
        "    z = tf.reshape( z, (-1, a_len, dim))\n",
        "    y = tf.matmul(z, tf.transpose(item_reviews, [0,2,1]))\n",
        "    if user_mask is not None and item_mask is not None:\n",
        "        mat_mask = tf.matmul(tf.expand_dims(user_mask,2), tf.expand_dims(item_mask,1))\n",
        "    if pooling == 'MAX':\n",
        "      if (user_mask is not None and item_mask is not None):\n",
        "            y = -1E+30 * (1-mat_mask) + y\n",
        "            att_row = tf.math.reduce_max(y,1)\n",
        "            att_col = tf.math.reduce_max(y,2)\n",
        "    elif pooling == 'MEAN':\n",
        "       if (user_mask is not None and item_mask is not None):\n",
        "            y = y * mat_mask\n",
        "            att_row = tf.math.reduce_mean(y,1)\n",
        "            att_col = tf.math.reduce_mean(y,2)\n",
        "    if (user_mask is not None and item_mask is not None):\n",
        "            att_row = -1E+30 * (1-item_mask) + att_row\n",
        "            att_col = -1E+30 * (1-user_mask) + att_col\n",
        "\n",
        "    \n",
        "    _sa2 = tf.nn.softmax(att_row)\n",
        "    _sa1 = tf.nn.softmax(att_col)\n",
        "\n",
        "    if (gumbel):\n",
        "        att_row = self.gumbel_softmax(att_row, temp, hard=hard)\n",
        "        att_col = self.gumbel_softmax(att_col, temp, hard=hard)\n",
        "    else:\n",
        "        att_row = tf.nn.softmax(att_row)\n",
        "        att_col = tf.nn.softmax(att_col)\n",
        "    \n",
        "    _a2 = att_row\n",
        "    _a1 = att_col\n",
        "        \n",
        "    att_col = tf.expand_dims(att_col, 2)\n",
        "    att_row = tf.expand_dims(att_row, 2)\n",
        "\n",
        "    final_a = att_col * user_reviews\n",
        "    final_b = att_row * item_reviews\n",
        "\n",
        "    if self.dropout_user is not None:\n",
        "      final_a = self.dropout_user(final_a)\n",
        "      final_b = self.dropout_item(final_b)\n",
        "\n",
        "\n",
        "    return final_a, final_b, _a1, _a2, _sa1, _sa2, y\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeziW0pOBBhW"
      },
      "source": [
        "\n",
        "class MultiPointerCoattentionNetworks(tf.keras.layers.Layer):\n",
        "  def __init__(self, args, vocab_size, input_size=50, dropout=0.2):\n",
        "    super(MultiPointerCoattentionNetworks,self).__init__()\n",
        "    self.args = args\n",
        "    self.vocab_size = vocab_size\n",
        "    self.co_attention_review_lvl = Coattention(input_size, self.args,dropout)\n",
        "    self.co_attention_concept_lvl = Coattention(input_size,self.args, dropout)\n",
        "    self.concept_user_embeddings = tf.keras.layers.Embedding(self.vocab_size,self.args.emb_size,embeddings_initializer='glorot_uniform',embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.concept_item_embeddings = tf.keras.layers.Embedding(self.vocab_size,self.args.emb_size,embeddings_initializer='glorot_uniform',embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    \n",
        "    self.fn1 = tf.keras.layers.Dense(self.args.emb_size,kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.fn2 = tf.keras.layers.Dense(self.args.emb_size,kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    \n",
        "    self.depth = self.args.emb_size // self.args.self_num_heads\n",
        "    self.uwq = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    self.uwk = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    self.uwv = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    \n",
        "    self.udense = tf.keras.layers.Dense(self.args.emb_size)\n",
        "\n",
        "    self.iwq = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    self.iwk = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    self.iwv = tf.keras.layers.Dense(self.args.emb_size)\n",
        "    \n",
        "    self.idense = tf.keras.layers.Dense(self.args.emb_size)\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.args.self_num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, user_reviews, item_reviews, user_length,\n",
        "           item_length, o1_embed, o2_embed, o1_len, o2_len,\n",
        "           c1_inputs, c1_len, c2_inputs, c2_len, num_heads=2,\n",
        "           rnn_type='RAW_MSE_CAML_FN_FM'):\n",
        "    self.hatt1, self.hatt2 = [], [] \n",
        "    self.att1, self.att2 = [] , []\n",
        "    self.word_att1, self.word_att2 = [], []\n",
        "    f1, f2 = [], []\n",
        "    self.afm = []\n",
        "    self.afm2 = []\n",
        "    self.word_u = []\n",
        "    self.word_i = []\n",
        "    user_mask = tf.sequence_mask(user_length, self.args.dmax, dtype = tf.float32)\n",
        "    item_mask = tf.sequence_mask(item_length, self.args.dmax, dtype = tf.float32)\n",
        "    \n",
        "    batch_size = tf.shape(user_reviews)[0]\n",
        "    \n",
        "    uq = self.uwq(user_reviews)  # (batch_size, seq_len, d_model)\n",
        "    uk = self.uwk(user_reviews)  # (batch_size, seq_len, d_model)\n",
        "    uv = self.uwv(user_reviews)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    uq = self.split_heads(uq, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    uk = self.split_heads(uk, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    uv = self.split_heads(uv, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    #print(uq.shape,user_mask.shape)\n",
        "    user_scaled_attention, _ = scaled_dot_product_attention(\n",
        "        uq, uk, uv, user_mask)\n",
        "    \n",
        "    user_scaled_attention = tf.transpose(user_scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    user_concat_attention = tf.reshape(user_scaled_attention, \n",
        "                                  (batch_size, -1, self.args.emb_size))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    user_reviews = self.udense(user_concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    iq = self.iwq(item_reviews)  # (batch_size, seq_len, d_model)\n",
        "    ik = self.iwk(item_reviews)  # (batch_size, seq_len, d_model)\n",
        "    iv = self.iwv(item_reviews)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    iq = self.split_heads(iq, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    ik = self.split_heads(ik, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    iv = self.split_heads(iv, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    \n",
        "    item_scaled_attention, _ = scaled_dot_product_attention(\n",
        "        iq, ik, iv, item_mask)\n",
        "    \n",
        "    item_scaled_attention = tf.transpose(item_scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    item_concat_attention = tf.reshape(item_scaled_attention, \n",
        "                                  (batch_size, -1, self.args.emb_size))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    item_reviews = self.idense(item_concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "\n",
        "    for i in range(num_heads):  \n",
        "      \n",
        "\n",
        "      attended_user, attended_item, att_col, att_row, soft_att_col, soft_att_row, afm = self.co_attention_review_lvl(\n",
        "          user_reviews, item_reviews, user_mask, item_mask, gumbel = True , pooling='MAX'\n",
        "      )\n",
        "\n",
        "      attended_user = tf.math.reduce_sum(attended_user, 1)\n",
        "      attended_item = tf.math.reduce_sum(attended_item, 1)\n",
        "      f1.append(attended_user)\n",
        "      f2.append(attended_item)\n",
        "      self.att1.append(soft_att_col)\n",
        "      self.att2.append(soft_att_row)\n",
        "      self.hatt1.append(att_col)\n",
        "      self.hatt2.append(att_row)\n",
        "      self.afm.append(afm)\n",
        "      \n",
        "\n",
        "      #print(\"=========================================\")\n",
        "      #print('Concept level attention')\n",
        "    \n",
        "      sub_afm=[]\n",
        "      _odim = o1_embed.get_shape().as_list()[2]\n",
        "      o1_embed = tf.reshape(o1_embed, [-1, self.args.dmax,\n",
        "                            self.args.smax * self.args.emb_size])\n",
        "      o2_embed = tf.reshape(o2_embed, [-1, self.args.dmax,\n",
        "                            self.args.smax * self.args.emb_size])\n",
        "      \n",
        "      _att_col = tf.expand_dims(att_col, 2)\n",
        "      _att_row = tf.expand_dims(att_row, 2)\n",
        "      \n",
        "      \n",
        "      review_concept1 = tf.reduce_sum(tf.reshape(c1_inputs, [-1,self.args.dmax, self.args.smax]) * tf.cast(_att_col, dtype = tf.int64),1)\n",
        "      review_concept2 = tf.reduce_sum(tf.reshape(c2_inputs, [-1,self.args.dmax, self.args.smax]) * tf.cast(_att_row, dtype = tf.int64),1)\n",
        "      \n",
        "      \n",
        "      _o1 = self.concept_user_embeddings(review_concept1)\n",
        "      _o2 = self.concept_item_embeddings(review_concept2)\n",
        "\n",
        "      _o1_len = tf.reshape(c1_len, [-1, self.args.dmax])\n",
        "      _o2_len = tf.reshape(c2_len, [-1, self.args.dmax])\n",
        "      _o1_len = tf.reduce_sum(_o1_len * tf.cast(att_col, tf.int64),1)\n",
        "      _o2_len = tf.reduce_sum(_o2_len * tf.cast(att_row, tf.int64),1)\n",
        "      _o1_len = tf.reshape(_o1_len,[-1])\n",
        "      _o2_len = tf.reshape(_o2_len, [-1])\n",
        "      o1_mask = tf.sequence_mask(_o1_len, self.args.smax, dtype=tf.float32)\n",
        "      o2_mask = tf.sequence_mask(_o2_len, self.args.smax, dtype=tf.float32)\n",
        "\n",
        "      attended_user_concept, attended_item_concept, att_col_concept, att_row_concept, soft_att_col_concept, soft_att_row_concept, afm_concept = self.co_attention_concept_lvl(\n",
        "          _o1, _o2, o1_mask, o2_mask, gumbel = True , pooling='MEAN'\n",
        "      )\n",
        "      sub_afm.append(afm_concept)\n",
        "      attended_user_concept = tf.reduce_sum(attended_user_concept,1)\n",
        "      attended_item_concept = tf.reduce_sum(attended_item_concept,1)\n",
        "\n",
        "      f1.append(attended_user_concept)\n",
        "      f2.append(attended_item_concept)\n",
        "\n",
        "      self.afm2.append(afm_concept)\n",
        "      self.word_att1.append(soft_att_col_concept)\n",
        "      self.word_att2.append(soft_att_row_concept)\n",
        "      word1 = tf.expand_dims(tf.reduce_sum(review_concept1 * tf.cast(att_col_concept, dtype=tf.int64), 1), 1)\n",
        "      word2 = tf.expand_dims(tf.reduce_sum(review_concept2 * tf.cast(att_row_concept, dtype=tf.int64), 1), 1)\n",
        "      self.word_u.append(word1)\n",
        "      self.word_i.append(word2)\n",
        "    \n",
        "    self.word_u = tf.concat(self.word_u, 1)\n",
        "    self.word_i = tf.concat(self.word_i, 1)\n",
        "    user_output = tf.concat(f1,1)\n",
        "    item_output = tf.concat(f2,1)\n",
        "    user_output = self.fn1(user_output)\n",
        "    item_output = self.fn2(item_output)\n",
        "\n",
        "    return user_output, item_output, self.word_u, self.word_i\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bSy5jt448jN"
      },
      "source": [
        "#Factorization Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emsXShYCjXUe"
      },
      "source": [
        "#@title\n",
        "class FactorizationMachine(tf.keras.layers.Layer):\n",
        "  def __init__(self, fm_p, fm_k=5, dropout=0.2):\n",
        "    super(FactorizationMachine, self).__init__()\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.initializer = tf.constant_initializer( value=0.0)\n",
        "    self.glorot_initializer = tf.keras.initializers.glorot_uniform()\n",
        "    self.fm_w0 = tf.Variable(self.initializer([1]))\n",
        "    self.fm_w = tf.Variable(self.initializer([fm_p]))\n",
        "    self.fm_V = tf.Variable(self.glorot_initializer([fm_k,fm_p]))\n",
        "\n",
        "    \n",
        "  def call(self, user_output, item_output):\n",
        "    \n",
        "    input_vec = tf.concat([user_output, item_output],1)\n",
        "    input_vec = self.dropout1(input_vec)\n",
        "    fm_linear_terms = self.fm_w0 +  tf.matmul(input_vec, tf.expand_dims(self.fm_w, 1))\n",
        "    fm_interactions_part1 = tf.matmul(input_vec, tf.transpose(self.fm_V))\n",
        "    fm_interactions_part1 = tf.pow(fm_interactions_part1, 2)\n",
        "    fm_interactions_part2 = tf.matmul(tf.pow(input_vec, 2),\n",
        "                                        tf.transpose(tf.pow(self.fm_V, 2)))\n",
        "    fm_interactions = fm_interactions_part1 - fm_interactions_part2\n",
        "    latent_dim = fm_interactions\n",
        "    fm_interactions = tf.reduce_sum(fm_interactions, 1, keepdims = True)\n",
        "    fm_interactions = tf.multiply(0.5, fm_interactions)\n",
        "    fm_prediction = tf.add(fm_linear_terms, fm_interactions)\n",
        "    return fm_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47yBLDTogDHB"
      },
      "source": [
        "class RatingPredictorMLP(tf.keras.layers.Layer):\n",
        "  def __init__(self,args):\n",
        "    super(RatingPredictorMLP, self).__init__()\n",
        "    self.args = args\n",
        "    self.fc1 = tf.keras.layers.Dense(100, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.fc2 = tf.keras.layers.Dense(50, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.fc3 = tf.keras.layers.Dense(10, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.fc4 = tf.keras.layers.Dense(1, activation='linear',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "\n",
        "  def call(self, user_output, item_output):\n",
        "    input_vec = tf.concat([user_output, item_output],1)\n",
        "    input_vec = self.fc1(input_vec)\n",
        "    input_vec = self.fc2(input_vec)\n",
        "    input_vec = self.fc3(input_vec)\n",
        "    input_vec = self.fc4(input_vec)\n",
        "    return input_vec\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-3Bfu50VIFz"
      },
      "source": [
        "#Model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3W_MK02S-Sx"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,args):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.args = args\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,self.args.emb_size,embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.gru = tf.keras.layers.GRU(self.args.rnn_size,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size,kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\n",
        "    self.dropout = tf.keras.layers.Dropout(args.dropout)\n",
        "  def call(self,x,hidden):\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(hidden, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    x = self.dropout(x)\n",
        "    return x,state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqAqQb_NOzD"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "class Model(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, args, \r\n",
        "                 char_vocab=0, pos_vocab=0,\r\n",
        "                 mode='RANK', num_user=10, num_item=10):\r\n",
        "        super(Model,self).__init__()\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.char_vocab = char_vocab\r\n",
        "        self.pos_vocab = pos_vocab\r\n",
        "        #self.graph = tf.Graph()\r\n",
        "        self.args = args\r\n",
        "      \r\n",
        "        self.inspect_op = []\r\n",
        "        self.mode=mode\r\n",
        "        self.write_dict = {}\r\n",
        "        self.PAD_tag = 0\r\n",
        "        self.SOS_tag = 2\r\n",
        "        self.EOS_tag = 3\r\n",
        "        self.UNK_tag = 1\r\n",
        "        # For interaction data only (disabled and removed from this repo)\r\n",
        "        self.num_user = num_user\r\n",
        "        self.num_item = num_item\r\n",
        "        print('Creating Model in [{}] mode'.format(self.mode))\r\n",
        "        self.feat_prop = None\r\n",
        "        \r\n",
        "        self.embeddings = tf.keras.layers.Embedding(self.vocab_size,self.args.emb_size,embeddings_initializer='glorot_uniform',embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.user_embeddings = tf.keras.layers.Embedding(self.num_user, self.args.latent_size, embeddings_initializer ='glorot_uniform',embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.item_embeddings = tf.keras.layers.Embedding(self.num_item, self.args.latent_size, embeddings_initializer='glorot_uniform',embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.user_dropout_1 = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        self.user_fc_1 = tf.keras.layers.Dense(self.args.rnn_size, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.user_dropout_2 = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        self.user_fc_2 = tf.keras.layers.Dense(self.args.rnn_size, activation ='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "\r\n",
        "        self.item_dropout_1 = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        self.item_fc_1 = tf.keras.layers.Dense(self.args.rnn_size, activation='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.item_dropout_2 = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        self.item_fc_2 = tf.keras.layers.Dense(self.args.rnn_size, activation ='relu',kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "  \r\n",
        "              \r\n",
        "        self.multi_pointer_coattention_networks = MultiPointerCoattentionNetworks(input_size=self.args.emb_size, args=args, vocab_size=self.vocab_size)\r\n",
        "        self.af_co_attend_user_dropout = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        self.af_co_attend_item_dropout = tf.keras.layers.Dropout(self.args.dropout)\r\n",
        "        #self.fm = FactorizationMachine(4*self.args.emb_size, fm_k=10, dropout=self.args.dropout)\r\n",
        "        self.mlp = RatingPredictorMLP(self.args)\r\n",
        "\r\n",
        "        #generate initial hidden state s0\r\n",
        "        self.review_user_mapping = tf.keras.layers.Dense(self.args.rnn_size,kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.review_item_mapping = tf.keras.layers.Dense(self.args.rnn_size,kernel_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "        self.review_rating_embedding = tf.keras.layers.Embedding(5,self.args.rnn_size,embeddings_regularizer= tf.keras.regularizers.L2(self.args.l2_reg))\r\n",
        "    def make_hmasks(self, inputs, smax):\r\n",
        "        # Hierarchical Masks\r\n",
        "        # Inputs are bsz x (dmax * smax)\r\n",
        "        inputs = tf.reshape(inputs,[-1, smax])\r\n",
        "        masked_inputs = tf.cast(inputs, tf.bool)\r\n",
        "        return masked_inputs\r\n",
        "\r\n",
        "    def learn_single_repr(self, q1_embed, q1_len, q1_max,\r\n",
        "                       pool=False, mask=None):\r\n",
        "      \r\n",
        "      if mask is not None:\r\n",
        "          masks = tf.cast(mask, tf.float32)\r\n",
        "          masks = tf.expand_dims(masks, 2)\r\n",
        "          masks = tf.tile(masks, [1,1, self.args.emb_size])\r\n",
        "          q1_embed = q1_embed * masks\r\n",
        "\r\n",
        "      q1_output = tf.reduce_sum(q1_embed, 1)\r\n",
        "      if(pool):\r\n",
        "          return q1_embed, q1_output\r\n",
        "      \r\n",
        "\r\n",
        "      return q1_embed, q1_output\r\n",
        "    \r\n",
        "    def call(self, user_reviews, user_length, item_reviews,\r\n",
        "             item_length, user_concepts, user_concepts_len, item_concepts, item_concepts_len, user_id, item_id,sig_labels,training=True):\r\n",
        "      self.user_reviews = user_reviews\r\n",
        "      self.item_reviews = item_reviews\r\n",
        "      self.user_length = user_length\r\n",
        "      self.item_length = item_length\r\n",
        "      self.user_concepts = user_concepts\r\n",
        "      self.user_concepts_len = user_concepts_len\r\n",
        "      self.item_concepts = item_concepts\r\n",
        "      self.item_concepts_len = item_concepts_len\r\n",
        "      self.user_batch = self.user_embeddings(user_id)\r\n",
        "      self.item_batch = self.item_embeddings(item_id)\r\n",
        "\r\n",
        "\r\n",
        "      #self.user_batch = self.user_embeddings(user_id)\r\n",
        "      #self.item_batch = self.item_embeddings(item_id)\r\n",
        "      self.user_reviews_mask = tf.cast(self.user_reviews, tf.bool)\r\n",
        "      self.item_reviews_mask = tf.cast(self.item_reviews, tf.bool)\r\n",
        "      self.user_reviews_hmask = self.make_hmasks(self.user_reviews, self.args.smax)\r\n",
        "      \r\n",
        "      self.item_reviews_hmask = self.make_hmasks(self.item_reviews, self.args.smax)\r\n",
        "      \r\n",
        "      user_reviews_embed = self.embeddings(self.user_reviews)\r\n",
        "      item_reviews_embed = self.embeddings(self.item_reviews)\r\n",
        "      #print(user_reviews_embed.shape)\r\n",
        "      #print(item_reviews_embed.shape)\r\n",
        "      user_reviews_embed, user_length = hierarchical_flatten(user_reviews_embed, self.user_length, self.args.smax)\r\n",
        "      item_reviews_embed, item_length = hierarchical_flatten(item_reviews_embed, self.item_length, self.args.smax) #(num_users * d_max x smax x embed_size)\r\n",
        "      self.o1_embed = user_reviews_embed\r\n",
        "      self.o2_embed = item_reviews_embed\r\n",
        "      self.o1_len = user_length\r\n",
        "      self.o2_len = item_length\r\n",
        "      #print(user_reviews_embed.shape)\r\n",
        "      #print(item_reviews_embed.shape)\r\n",
        "      _, user_reviews_embed = self.learn_single_repr(user_reviews_embed, user_length,\r\n",
        "                                                  self.args.base_encoder,pool=True, mask=self.user_reviews_hmask)\r\n",
        "      _, item_reviews_embed = self.learn_single_repr(item_reviews_embed, item_length,\r\n",
        "                                                  self.args.base_encoder,pool=True, mask=self.item_reviews_hmask)\r\n",
        "      _dim = user_reviews_embed.get_shape().as_list()[1]\r\n",
        "      self.user_reviews_embed = tf.reshape(user_reviews_embed, [-1, self.args.dmax, _dim])\r\n",
        "      self.item_reviews_embed = tf.reshape(item_reviews_embed, [-1, self.args.dmax, _dim])\r\n",
        "      #print(\"=================================================\")\r\n",
        "      #print(tf.cast(self.user_length, dtype='bool'))\r\n",
        "      user_length = tf.cast(tf.math.count_nonzero(self.user_length, axis =1), tf.int32)\r\n",
        "      item_length = tf.cast(tf.math.count_nonzero(self.item_length, axis =1), tf.int32)\r\n",
        "      #print(user_length, item_length)\r\n",
        "\r\n",
        "      #joint representation\r\n",
        "      user_reviews_embed = self.user_dropout_1(self.user_reviews_embed)\r\n",
        "      user_reviews_embed = self.user_fc_1(user_reviews_embed)\r\n",
        "      user_reviews_embed = self.user_dropout_2(user_reviews_embed)\r\n",
        "      user_reviews_embed = self.user_fc_2(user_reviews_embed)\r\n",
        "\r\n",
        "      item_reviews_embed = self.item_dropout_1(self.item_reviews_embed)\r\n",
        "      item_reviews_embed = self.item_fc_1(item_reviews_embed)\r\n",
        "      item_reviews_embed = self.item_dropout_2(item_reviews_embed)\r\n",
        "      item_reviews_embed = self.item_fc_2(item_reviews_embed)\r\n",
        "      \r\n",
        "      print('See here shape', user_reviews_embed.shape)\r\n",
        "      user_output, item_output, word_u, word_i = self.multi_pointer_coattention_networks(user_reviews_embed, item_reviews_embed,\r\n",
        "                                                             user_length, item_length, self.o1_embed,self.o2_embed,\r\n",
        "                                                             self.o1_len, self.o2_len, self.user_concepts, self.user_concepts_len, self.item_concepts,\r\n",
        "                                                             self.item_concepts_len,\r\n",
        "                                                             rnn_type = self.args.rnn_type, num_heads=self.args.num_heads\r\n",
        "                                                             )\r\n",
        "      \r\n",
        "      try:\r\n",
        "        self.max_norm = tf.reduce_max(tf.norm(user_output,\r\n",
        "                                              ord='euclidean',\r\n",
        "                                              keep_dims=True, axis=1))\r\n",
        "      except:\r\n",
        "        self.max_norm = 0\r\n",
        "\r\n",
        "      user_output = tf.concat([user_output, self.user_batch], 1)\r\n",
        "      item_output = tf.concat([item_output, self.item_batch], 1)\r\n",
        "\r\n",
        "      user_output = self.af_co_attend_user_dropout(user_output)\r\n",
        "      item_output = self.af_co_attend_item_dropout(item_output)\r\n",
        "\r\n",
        "      \r\n",
        "      #print('=========================================================================')\r\n",
        "      #print('FactorizationMachine')\r\n",
        "      #prediction = self.fm(user_output, item_output)\r\n",
        "      prediction = self.mlp(user_output, item_output)\r\n",
        "\r\n",
        "      #generate initial hidden state s0 without feeding rating\r\n",
        "      #move [1,5] -> [0,4]\r\n",
        "      r_input = tf.cast(sig_labels, dtype=tf.int64) - 1\r\n",
        "      #r_input = tf.clip_by_value(tf.cast(tf.reshape(prediction, [-1]), dtype=tf.int64), 1, 5)-1\r\n",
        "      r_embed = self.review_rating_embedding(r_input)\r\n",
        "      state = tf.keras.activations.tanh(r_embed+self.review_user_mapping(user_output)+self.review_item_mapping(item_output))\r\n",
        "      return prediction,state, word_u, word_i\r\n",
        "\r\n",
        "      # Inspired by code from https://github.com/3878anonymous/CAML/blob/master/tf_models/model_caml.py\r\n",
        "    def _beam_search_infer(self, q1_output, q2_output, r_input, reuse=None):\r\n",
        "          dim = q1_output.get_shape().as_list()[1]\r\n",
        "          \r\n",
        "\r\n",
        "          init= tf.initializers.GlorotUniform()\r\n",
        "          #cal state\r\n",
        "          self.review_user_mapping = tf.Variable(init(shape=(dim, self.args.rnn_dim)))\r\n",
        "                                                    \r\n",
        "\r\n",
        "          self.review_item_mapping = tf.Variable(init(shape=(dim, self.args.rnn_dim)))\r\n",
        "                                                    \r\n",
        "\r\n",
        "          \r\n",
        "\r\n",
        "          self.review_bias = tf.Variable(init(shape=(self.args.rnn_dim)))\r\n",
        "\r\n",
        "          self.rnn_cell = tf.compat.v2.nn.rnn_cell.GRUCell(self.args.rnn_dim)\r\n",
        "\r\n",
        "          #cal state\r\n",
        "          if r_input is not None:\r\n",
        "              r_embed = tf.nn.embedding_lookup(self.review_rating_embeddings, r_input)\r\n",
        "              state = tf.math.tanh(r_embed + tf.matmul(q1_output, self.review_user_mapping) + tf.matmul(q2_output, self.review_item_mapping) + self.review_bias)\r\n",
        "          else:\r\n",
        "              state = tf.math.tanh(tf.matmul(q1_output, self.review_user_mapping) + tf.matmul(q2_output, self.review_item_mapping) + self.review_bias)\r\n",
        "\r\n",
        "          self.beam_batch = self.args.beam_size * self.args.batch_size\r\n",
        "          self.beam_batch_max = self.args.beam_size * self.args.beam_size * self.args.batch_size\r\n",
        "\r\n",
        "          #max_val = self.max_val\r\n",
        "          #initializer = tf.random_uniform_initializer(-max_val, max_val, dtype=self.dtype)\r\n",
        "\r\n",
        "          neg_words = tf.Variable(tf.constant(0.0, shape=[self.vocab_size]), trainable=False)\r\n",
        "          neg_words = tf.reshape(tf.scatter_update(neg_words, tf.constant(self.UNK_tag), tf.constant(1.0)), shape=[1, self.vocab_size])\r\n",
        "\r\n",
        "          neg_words_batch = tf.tile(neg_words, [self.args.batch_size, 1])\r\n",
        "          neg_words_beam_batch = tf.tile(neg_words, [self.beam_batch, 1])\r\n",
        "\r\n",
        "          pad_batch = tf.constant(self.PAD_tag, shape=[self.beam_batch])\r\n",
        "          eos_batch = tf.constant(self.EOS_tag, shape=[self.beam_batch])\r\n",
        "          min_batch = tf.constant(-100.0, shape=[self.beam_batch])\r\n",
        "          zero_batch = tf.constant(0.0, shape=[self.beam_batch])\r\n",
        "\r\n",
        "          #pad_batch_max = tf.constant(self.PAD_tag, shape=[self.beam_batch_max])\r\n",
        "          eos_batch_max = tf.constant(self.EOS_tag, shape=[self.beam_batch_max])\r\n",
        "\r\n",
        "          \r\n",
        "          final_input = state\r\n",
        "\r\n",
        "          \r\n",
        "          logits = tf.compat.v1.layers.dense(final_input, self.vocab_size, kernel_initializer=self.initializer, bias_initializer=self.initializer, name='review_output_layer')\r\n",
        "          self.preds = tf.nn.softmax(logits) - neg_words_batch\r\n",
        "          values, indices = tf.nn.top_k(self.preds, self.args.beam_size)\r\n",
        "\r\n",
        "          init_ans = tf.reshape(indices, shape=[self.beam_batch, 1])\r\n",
        "          init_loss = tf.math.log(tf.reshape(values, shape=[self.beam_batch]))\r\n",
        "          init_tag = tf.cast(tf.equal(tf.reshape(indices, shape=[self.beam_batch]), eos_batch), tf.int32)\r\n",
        "          init_end_tag = tf.reduce_sum(init_tag, axis=None)\r\n",
        "          sum_tag = tf.constant(self.beam_batch, dtype=tf.int32)\r\n",
        "          max_length = tf.constant(self.args.gmax, dtype=tf.int32)\r\n",
        "\r\n",
        "          init_len = tf.constant(1, shape=[self.beam_batch])\r\n",
        "\r\n",
        "          init_lm_inputs = tf.reshape(indices, shape=[self.beam_batch])\r\n",
        "          init_state = tf.reshape(tf.tile(state, [1, self.args.beam_size]), shape = [self.beam_batch, self.args.rnn_dim])\r\n",
        "\r\n",
        "          \r\n",
        "          def condition(end_tag, tag, answer, lens, *args):\r\n",
        "              return tf.logical_and(end_tag < sum_tag, tf.shape(answer)[1]<= max_length)\r\n",
        "\r\n",
        "          def forward_one_step(end_tag, tag, answer, lens, loss, lm_inputs, state):\r\n",
        "              self.tip_inputs_embedded = tf.nn.embedding_lookup(\r\n",
        "                      params=self.embeddings, ids=lm_inputs)\r\n",
        "\r\n",
        "              \r\n",
        "              self.rnnlm_outputs, new_state = self.rnn_cell(self.tip_inputs_embedded, state)\r\n",
        "\r\n",
        "              #new_tag = 1 - tag\r\n",
        "              loss_old_pad = tf.where(tf.cast(tag, tf.bool), zero_batch, min_batch)\r\n",
        "              loss_old = loss + loss_old_pad\r\n",
        "              tag_old = tag\r\n",
        "              lens_old = lens\r\n",
        "              state_old = state\r\n",
        "              answer_old = tf.concat([answer, tf.reshape(pad_batch, shape=[self.beam_batch, 1])], axis=1)\r\n",
        "\r\n",
        "              loss_new_pad = tf.where(tf.cast(tag, tf.bool), min_batch, zero_batch)\r\n",
        "              loss_new = loss + loss_new_pad\r\n",
        "              loss_new = tf.reshape(tf.tile(tf.reshape(loss_new, shape=[self.beam_batch, 1]), [1, self.args.beam_size]), shape = [self.beam_batch_max])\r\n",
        "\r\n",
        "              \r\n",
        "              final_input = self.rnnlm_outputs\r\n",
        "              \r\n",
        "              logits = tf.compact.v1.layers.dense(final_input, self.vocab_size, kernel_initializer=self.initializer, bias_initializer=self.initializer, reuse=True)\r\n",
        "              \r\n",
        "              self.preds = tf.nn.softmax(logits) - neg_words_beam_batch\r\n",
        "              values, indices = tf.nn.top_k(self.preds, self.args.beam_size)\r\n",
        "\r\n",
        "              values = tf.reshape(values, shape=[self.beam_batch_max])\r\n",
        "              loss_new = loss_new + tf.log(values)\r\n",
        "\r\n",
        "              answer_new = tf.reshape(tf.tile(answer, [1, self.args.beam_size]), shape=[self.beam_batch_max, -1])\r\n",
        "              indices = tf.reshape(indices, shape=[self.beam_batch_max])\r\n",
        "              answer_new = tf.concat([answer_new, tf.reshape(indices, shape=[self.beam_batch_max, 1])], axis=1)\r\n",
        "\r\n",
        "              state_new = tf.reshape(tf.tile(new_state, [1, self.args.beam_size]), shape=[self.beam_batch_max, -1])\r\n",
        "              tag_new = tf.cast(tf.equal(indices, eos_batch_max), tf.int32)\r\n",
        "              lens_new = tf.reshape(tf.tile(tf.reshape(lens, shape=[self.beam_batch, 1]), [1, self.args.beam_size]), shape=[self.beam_batch_max])\r\n",
        "              lens_new = lens_new + 1\r\n",
        "\r\n",
        "              #merge\r\n",
        "              merge_tag = tf.concat([tf.reshape(tag_old, shape=[self.args.batch_size, -1]), tf.reshape(tag_new, shape=[self.args.batch_size, -1])], axis=1)\r\n",
        "              merge_lens = tf.concat([tf.reshape(lens_old, shape=[self.args.batch_size, -1]), tf.reshape(lens_new, shape=[self.args.batch_size, -1])], axis=1)\r\n",
        "              merge_state = tf.concat([tf.reshape(state_old, shape=[self.args.batch_size, self.args.beam_size, -1]), tf.reshape(state_new, shape=[self.args.batch_size, self.args.beam_size * self.args.beam_size, -1])], axis=1)\r\n",
        "              merge_loss = tf.concat([tf.reshape(loss_old, shape=[self.args.batch_size, -1]), tf.reshape(loss_new, shape=[self.args.batch_size, -1])], axis=1)\r\n",
        "              #average_loss = tf.div(merge_loss, merge_lens)\r\n",
        "              merge_answer =  tf.concat([tf.reshape(answer_old, shape=[self.args.batch_size, self.args.beam_size, -1]), tf.reshape(answer_new, shape=[self.args.batch_size, self.args.beam_size * self.args.beam_size, -1])], axis=1)\r\n",
        "              merge_inputs = tf.concat([tf.reshape(lm_inputs, shape=[self.args.batch_size, -1]), tf.reshape(indices, shape=[self.args.batch_size, -1])], axis=1)\r\n",
        "\r\n",
        "              merge_values, merge_indices = tf.nn.top_k(merge_loss, self.args.beam_size)\r\n",
        "\r\n",
        "              #new_loss = tf.reshape(merge_values, shape=[self.beam_batch])\r\n",
        "              merge_indices = tf.reshape(merge_indices, shape=[self.beam_batch, 1])\r\n",
        "              range_batch = tf.reshape(tf.tile(tf.reshape(tf.range(self.args.batch_size), shape=[self.args.batch_size, 1]), [1, self.args.beam_size]), shape=[self.beam_batch, 1])\r\n",
        "              index = tf.concat([range_batch, merge_indices], axis=1)\r\n",
        "\r\n",
        "              new_loss = tf.reshape(tf.gather_nd(merge_loss, index), shape=[self.beam_batch])\r\n",
        "              new_tag = tf.reshape(tf.gather_nd(merge_tag, index), shape=[self.beam_batch])\r\n",
        "              new_lens = tf.reshape(tf.gather_nd(merge_lens, index), shape=[self.beam_batch])\r\n",
        "              new_state = tf.reshape(tf.gather_nd(merge_state, index), shape=[self.beam_batch, -1])\r\n",
        "              new_answer = tf.reshape(tf.gather_nd(merge_answer, index), shape=[self.beam_batch, -1])\r\n",
        "              new_inputs = tf.reshape(tf.gather_nd(merge_inputs, index), shape=[self.beam_batch])\r\n",
        "\r\n",
        "              sum_end = tf.reduce_sum(new_tag, axis=None)\r\n",
        "\r\n",
        "              return sum_end, new_tag, new_answer, new_lens, new_loss, new_inputs, new_state\r\n",
        "\r\n",
        "          sum_end, tag, answer, lens, loss, lm_inputs, state = tf.while_loop(condition, forward_one_step, [init_end_tag, init_tag, init_ans, init_len, init_loss, init_lm_inputs, init_state],  shape_invariants=[init_end_tag.get_shape(), init_tag.get_shape(), tf.TensorShape([self.beam_batch, None]), init_len.get_shape(), init_loss.get_shape(), init_lm_inputs.get_shape(), init_state.get_shape()])\r\n",
        "\r\n",
        "          return answer\r\n",
        "        \r\n",
        "\r\n",
        "    \r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PD8daGkLGTN"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  #print(real)\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss_ = loss_object(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "  loss_ *= mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss_)\r\n",
        "\r\n",
        "def _cal_key_loss(args, pred, word_u, word_i):\r\n",
        "  preds = tf.nn.softmax(pred)\r\n",
        "  preds = -tf.math.log(preds + 1e-7)\r\n",
        "  word = tf.concat([word_u, word_i], 1)\r\n",
        "  num_words = word.shape[1]\r\n",
        "  prefix = tf.range(args.batch_size)\r\n",
        "  prefix = tf.tile(tf.expand_dims(prefix,1), [1, num_words])\r\n",
        "  word = tf.cast(word,dtype='int32')\r\n",
        "  indices = tf.concat([tf.expand_dims(prefix, 2), tf.expand_dims(word, 2)], 2)\r\n",
        "  l = tf.gather_nd(preds, indices)\r\n",
        "  if args.word_aggregate == 'MEAN':\r\n",
        "            l = tf.reduce_mean(l, 1)\r\n",
        "  else:\r\n",
        "      if args.word_aggregate == 'MAX':\r\n",
        "        l = tf.reduce_max(l, 1)\r\n",
        "      else:\r\n",
        "        l = tf.reduce_min(l, 1)\r\n",
        "\r\n",
        "  l = args.key_word_lambda * tf.reduce_mean(l)\r\n",
        "\r\n",
        "  return l\r\n",
        "\r\n",
        "\r\n",
        "class CFExperiment(Experiment):\r\n",
        "    \"\"\" Main experiment class for collaborative filtering.\r\n",
        "    Check tylib/exp/experiment.py for base class.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self, inject_params=None, preprocessed=False):\r\n",
        "        print(\"Starting Rec Experiment\")\r\n",
        "        super(CFExperiment, self).__init__()\r\n",
        "        self.uuid = datetime.now().strftime(\"%d:%m:%H:%M:%S\")\r\n",
        "        #self.parser = build_parser()\r\n",
        "        self.no_text_mode = False\r\n",
        "\r\n",
        "        #self.args = self.parser.parse_args()\r\n",
        "        self.args= args()\r\n",
        "        #print(self.args.l2_reg)\r\n",
        "        self.max_val = 5\r\n",
        "        self.min_val = 1\r\n",
        "\r\n",
        "        self.show_metrics = ['MSE','RMSE','MAE', 'MSE_int','RMSE_int','MAE_int', 'Gen_loss', 'All_loss', 'Gen_loss', 'F1', 'ACC', 'Review_acc']\r\n",
        "        #self.eval_primary = 'RMSE'\r\n",
        "        self.eval_primary = 'All_loss'\r\n",
        "        # For hierarchical setting\r\n",
        "        self.args.qmax = self.args.smax * self.args.dmax\r\n",
        "        self.args.amax = self.args.smax * self.args.dmax\r\n",
        "\r\n",
        "        print(\"Setting up environment..\")\r\n",
        "\r\n",
        "        #self.model_wrapper()\r\n",
        "        self.model_name = 'RAW_MSE_CAML_FN_FM'\r\n",
        "        self._setup()\r\n",
        "        print(preprocessed)\r\n",
        "        if (preprocessed==False):\r\n",
        "          self._load_sets()\r\n",
        "          self.train_data = self._combine_reviews(self.train_rating_set, self.train_reviews)\r\n",
        "          #self.test_set = self._combine_reviews(self.test_rating_set, self.test_reviews)\r\n",
        "          #self.dev_set = self._combine_reviews(self.dev_rating_set, self.dev_reviews)\r\n",
        "          self.data = self._prepare_set(self.train_data)\r\n",
        "          self.num_user = max(self.ui_review_dict.keys())+1\r\n",
        "          self.num_item = max(self.iu_review_dict.keys())+1\r\n",
        "          # Saving preprocessed data\r\n",
        "          saved_data = {'data':self.data,'vocab':self.vocab,'num_user':self.num_user, 'num_item':self.num_item}\r\n",
        "          with open('/content/drive/MyDrive/CAML_TOY_DATA/movie_100/preprocessed_dict.bin','wb') as f:\r\n",
        "            pickle.dump(saved_data,f)\r\n",
        "        else:\r\n",
        "          with open('/content/drive/MyDrive/CAML_TOY_DATA/movie_100/preprocessed_dict.bin','rb') as f:\r\n",
        "            saved_data = pickle.load(f)\r\n",
        "            self.data = saved_data['data']\r\n",
        "            self.vocab = saved_data['vocab']\r\n",
        "            self.num_user = saved_data['num_user']\r\n",
        "            self.num_item = saved_data['num_item']\r\n",
        "      \r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "    def _load_sets(self):\r\n",
        "        # Load train, test and dev sets\r\n",
        "\r\n",
        "        data_link = '/content/drive/MyDrive/CAML_TOY_DATA/movie_100'\r\n",
        "\r\n",
        "        if(self.no_text_mode==False):\r\n",
        "            self.word_index = self.load_vocab(data_link)\r\n",
        "            self.index_word = {k:v for v, k in self.word_index.items()}\r\n",
        "\r\n",
        "            self.stop_concept = {}\r\n",
        "            frequent_words = 100\r\n",
        "            for i in range(frequent_words + 4):\r\n",
        "                self.stop_concept[self.index_word[i]] = 1\r\n",
        "            self.vocab = len(self.word_index)\r\n",
        "            print(\"vocab={}\".format(self.vocab))\r\n",
        "            #self.word2df = None\r\n",
        "\r\n",
        "\r\n",
        "        self.train_rating_set, self.train_reviews = self.load_dataset(data_link, 'train')\r\n",
        "        self.dev_rating_set, self.dev_reviews = self.load_dataset(data_link, 'valid')\r\n",
        "\r\n",
        "        if(self.args.dev==0):\r\n",
        "            self.train_rating_set += self.dev_rating_set\r\n",
        "        self.test_rating_set, self.test_reviews = self.load_dataset(data_link, 'test')\r\n",
        "\r\n",
        "        if(self.no_text_mode==False):\r\n",
        "\r\n",
        "            #load_reviews\r\n",
        "            self.ui_review_dict, self.iu_review_dict = self.load_review_data(data_link, \"review\")\r\n",
        "            self.ui_concept_dict, self.iu_concept_dict = self.load_review_data(data_link, \"concepts\")\r\n",
        "            #self.num_users = len(self.ui_review_dict)\r\n",
        "            #self.num_items = len(self.iu_review_dict)\r\n",
        "            self.num_users = max(self.ui_review_dict.keys()) + 1\r\n",
        "            self.num_items = max(self.iu_review_dict.keys()) + 1\r\n",
        "\r\n",
        "\r\n",
        "        self.write_to_file(\"Train={} Dev={} Test={}\".format(\r\n",
        "                                len(self.train_rating_set),\r\n",
        "                                len(self.dev_rating_set),\r\n",
        "                                len(self.test_rating_set)))\r\n",
        "\r\n",
        "    def load_vocab(self, data_dir):\r\n",
        "        lines_vocab = open('%s/vocabulary.txt' % data_dir, 'r', encoding='utf-8').readlines()\r\n",
        "\r\n",
        "        vocab = {}\r\n",
        "        for i,word in enumerate(lines_vocab):\r\n",
        "            vocab[word.strip()] = i + 4\r\n",
        "        vocab[PAD] = 0\r\n",
        "        vocab[UNK] = 1\r\n",
        "        vocab[SOS] = 2\r\n",
        "        vocab[EOS] = 3\r\n",
        "        \r\n",
        "        return vocab\r\n",
        "\r\n",
        "    def load_dataset(self, data_dir, dataset_type):\r\n",
        "        output = []\r\n",
        "        lines_user_id = open('%s/%s_userid.txt' % (data_dir, dataset_type), 'r', encoding='utf-8').readlines()\r\n",
        "        lines_item_id = open('%s/%s_itemid.txt' % (data_dir, dataset_type), 'r', encoding='utf-8').readlines()\r\n",
        "        lines_rating = open('%s/%s_rating.txt' % (data_dir, dataset_type), 'r', encoding='utf-8').readlines()\r\n",
        "        lines_review = open('%s/%s_review_1.txt' % (data_dir, dataset_type), 'r', encoding='utf-8').readlines()\r\n",
        "\r\n",
        "        reviews = []\r\n",
        "\r\n",
        "        concept_dict = {}\r\n",
        "        for key in self.word_index:\r\n",
        "            words = key.split(' ')\r\n",
        "            l = len(words)\r\n",
        "            for i in range(l - 1):\r\n",
        "                concept_dict[\" \".join(words[:l - i])] = 1\r\n",
        "\r\n",
        "        for i in range(len(lines_rating)):\r\n",
        "            output.append([int(lines_user_id[i].strip()), int(lines_item_id[i].strip()), int(lines_rating[i].strip())])\r\n",
        "\r\n",
        "            linedata = []\r\n",
        "            line = lines_review[i].strip()\r\n",
        "            line = line.split('\\t')\r\n",
        "\r\n",
        "            linedata.append(self.word_index[EOS])\r\n",
        "            l = len(line)\r\n",
        "            pos = l\r\n",
        "            while 1:\r\n",
        "                pos = pos - 1\r\n",
        "                if pos < 0:\r\n",
        "                    break\r\n",
        "\r\n",
        "                match_string = line[pos]\r\n",
        "                new_pos = pos\r\n",
        "                for j in range(pos):\r\n",
        "                    if (\" \".join(line[pos - j - 1: pos + 1]) in concept_dict):\r\n",
        "                        if (\" \".join(line[pos - j - 1: pos + 1]) in self.word_index):\r\n",
        "                            match_string = \" \".join(line[pos - j - 1: pos + 1])\r\n",
        "                            new_pos = pos - j - 1\r\n",
        "                        continue\r\n",
        "                    else:\r\n",
        "                        break\r\n",
        "                if match_string in self.word_index:\r\n",
        "                    linedata.append(self.word_index[match_string])\r\n",
        "                else:\r\n",
        "                    linedata.append(self.word_index[UNK])\r\n",
        "                pos = new_pos\r\n",
        "\r\n",
        "            linedata.append(self.word_index[SOS])\r\n",
        "            linedata = linedata[::-1]\r\n",
        "\r\n",
        "            reviews.append(linedata)\r\n",
        "\r\n",
        "        return output, reviews\r\n",
        "\r\n",
        "    def load_review_data(self, data_dir, data_type):\r\n",
        "        lines_user_id = open('%s/train_userid.txt' % data_dir, 'r', encoding='utf-8').readlines()\r\n",
        "        lines_item_id = open('%s/train_itemid.txt' % data_dir, 'r', encoding='utf-8').readlines()\r\n",
        "        lines_review = open('%s/train_%s.txt' % (data_dir, data_type), 'r', encoding='utf-8').readlines()\r\n",
        "\r\n",
        "        ui_dict = {}\r\n",
        "        iu_dict = {}\r\n",
        "\r\n",
        "        stop_concept = self.stop_concept\r\n",
        "\r\n",
        "        for i in range(len(lines_review)):\r\n",
        "            user = int(lines_user_id[i].strip())\r\n",
        "            item = int(lines_item_id[i].strip())\r\n",
        "\r\n",
        "            linedata = []\r\n",
        "            line = lines_review[i].strip()\r\n",
        "            if not (len(line) == 0):\r\n",
        "                line = line.split('\\t')\r\n",
        "                for j in range(len(line)):\r\n",
        "                    if line[j] in stop_concept:\r\n",
        "                        if data_type == \"concepts\":\r\n",
        "                            continue\r\n",
        "                    if line[j] in self.word_index:\r\n",
        "                        linedata.append(self.word_index[line[j]])\r\n",
        "                    else:\r\n",
        "                        linedata.append(self.word_index[UNK])\r\n",
        "                if len(linedata)>self.args.smax:\r\n",
        "                   linedata = linedata[:self.args.smax]\r\n",
        "\r\n",
        "            if user not in ui_dict:\r\n",
        "                ui_dict[user] = {}\r\n",
        "            if item not in iu_dict:\r\n",
        "                iu_dict[item] = {}\r\n",
        "\r\n",
        "            ui_dict[user][item] = linedata\r\n",
        "            iu_dict[item][user] = linedata\r\n",
        "\r\n",
        "        length1 = [len(ui_dict[x]) for x in ui_dict]\r\n",
        "        length2 = [len(iu_dict[x]) for x in iu_dict]\r\n",
        "        length3 = []\r\n",
        "        for x in ui_dict:\r\n",
        "         length3 += [len(ui_dict[x][y]) for y in ui_dict[x]]\r\n",
        "        show_stats('{}:user num review'.format(data_type), length1)\r\n",
        "        show_stats('{}:item num review'.format(data_type), length2)\r\n",
        "        show_stats('{}:review num word'.format(data_type), length3)\r\n",
        "\r\n",
        "        return ui_dict, iu_dict\r\n",
        "\r\n",
        "    def _combine_reviews(self, data, reviews = None):\r\n",
        "        user = [x[0] for x in data]\r\n",
        "        items = [x[1] for x in data]\r\n",
        "        labels = [x[2] for x in data]\r\n",
        "\r\n",
        "        #prep generation outputs\r\n",
        "        if reviews != None:\r\n",
        "\r\n",
        "            gen_outputs, gen_len = prep_data_list(reviews, self.args.gmax)\r\n",
        "\r\n",
        "        output = []\r\n",
        "        for i in range(len(user)):\r\n",
        "            output.append([user[i], items[i], labels[i], gen_outputs[i], gen_len[i]])\r\n",
        "\r\n",
        "        return output\r\n",
        "\r\n",
        "    def _prepare_set(self, data):\r\n",
        "\r\n",
        "        user = [x[0] for x in data]\r\n",
        "        items = [x[1] for x in data]\r\n",
        "        labels = [x[2] for x in data]\r\n",
        "\r\n",
        "        # Raw user-item ids\r\n",
        "        user_idx = user\r\n",
        "        item_idx = items\r\n",
        "\r\n",
        "        user_list = []\r\n",
        "        item_list = []\r\n",
        "        user_concept_list = []\r\n",
        "        item_concept_list = []\r\n",
        "        user_len = []\r\n",
        "        item_len = []\r\n",
        "        user_concept_len = []\r\n",
        "        item_concept_len = []\r\n",
        "        for i in range(len(user)):\r\n",
        "            user_reviews = []\r\n",
        "            item_reviews = []\r\n",
        "            user_concepts = []\r\n",
        "            item_concepts = []\r\n",
        "            user_r_len = []\r\n",
        "            user_c_len = []\r\n",
        "            item_r_len = []\r\n",
        "            item_c_len = []\r\n",
        "\r\n",
        "            if self.args.data_prepare == 1:\r\n",
        "                if items[i] in self.ui_review_dict[user[i]]:\r\n",
        "                    user_reviews.append(self.ui_review_dict[user[i]][items[i]])\r\n",
        "                    user_concepts.append(self.ui_concept_dict[user[i]][items[i]])\r\n",
        "                    user_r_len.append(len(self.ui_review_dict[user[i]][items[i]]))\r\n",
        "                    user_c_len.append(len(self.ui_concept_dict[user[i]][items[i]]))\r\n",
        "                for x in self.ui_review_dict[user[i]]:\r\n",
        "                    if not x==items[i]:\r\n",
        "                        user_reviews.append(self.ui_review_dict[user[i]][x])\r\n",
        "                        user_concepts.append(self.ui_concept_dict[user[i]][x])\r\n",
        "                        user_r_len.append(len(self.ui_review_dict[user[i]][x]))\r\n",
        "                        user_c_len.append(len(self.ui_concept_dict[user[i]][x]))\r\n",
        "                        if len(user_reviews) == self.args.dmax:\r\n",
        "                            break\r\n",
        "                if user[i] in self.iu_review_dict[items[i]]:\r\n",
        "                    item_reviews.append(self.iu_review_dict[items[i]][user[i]])\r\n",
        "                    item_concepts.append(self.iu_concept_dict[items[i]][user[i]])\r\n",
        "                    item_r_len.append(len(self.iu_review_dict[items[i]][user[i]]))\r\n",
        "                    item_c_len.append(len(self.iu_concept_dict[items[i]][user[i]]))\r\n",
        "                for x in self.iu_review_dict[items[i]]:\r\n",
        "                    if not x==user[i]:\r\n",
        "                        item_reviews.append(self.iu_review_dict[items[i]][x])\r\n",
        "                        item_concepts.append(self.iu_concept_dict[items[i]][x])\r\n",
        "                        item_r_len.append(len(self.iu_review_dict[items[i]][x]))\r\n",
        "                        item_c_len.append(len(self.iu_concept_dict[items[i]][x]))\r\n",
        "                        if len(item_reviews) == self.args.dmax:\r\n",
        "                            break\r\n",
        "                user_list.append(user_reviews)\r\n",
        "                item_list.append(item_reviews)\r\n",
        "                user_concept_list.append(user_concepts)\r\n",
        "                item_concept_list.append(item_concepts)\r\n",
        "                user_len.append(user_r_len)\r\n",
        "                item_len.append(item_r_len)\r\n",
        "                user_concept_len.append(user_c_len)\r\n",
        "                item_concept_len.append(item_c_len)\r\n",
        "            elif self.args.data_prepare == -1:\r\n",
        "                tmp = len(self.ui_review_dict[user[i]])\r\n",
        "                for x in self.ui_review_dict[user[i]]:\r\n",
        "                    if (not x==items[i]):\r\n",
        "                        user_reviews.append(self.ui_review_dict[user[i]][x])\r\n",
        "                        user_concepts.append(self.ui_concept_dict[user[i]][x])\r\n",
        "                        user_r_len.append(len(self.ui_review_dict[user[i]][x]))\r\n",
        "                        user_c_len.append(len(self.ui_concept_dict[user[i]][x]))\r\n",
        "                        if len(user_reviews) == self.args.dmax:\r\n",
        "                            break\r\n",
        "                tmp = len(self.iu_review_dict[items[i]])\r\n",
        "                for x in self.iu_review_dict[items[i]]:\r\n",
        "                    if (not x==user[i]):\r\n",
        "                        item_reviews.append(self.iu_review_dict[items[i]][x])\r\n",
        "                        item_concepts.append(self.iu_concept_dict[items[i]][x])\r\n",
        "                        item_r_len.append(len(self.iu_review_dict[items[i]][x]))\r\n",
        "                        item_c_len.append(len(self.iu_concept_dict[items[i]][x]))\r\n",
        "                        if len(item_reviews) == self.args.dmax:\r\n",
        "                            break\r\n",
        "                user_list.append(user_reviews)\r\n",
        "                item_list.append(item_reviews)\r\n",
        "                user_concept_list.append(user_concepts)\r\n",
        "                item_concept_list.append(item_concepts)\r\n",
        "                user_len.append(user_r_len)\r\n",
        "                item_len.append(item_r_len)\r\n",
        "                user_concept_len.append(user_c_len)\r\n",
        "                item_concept_len.append(item_c_len)\r\n",
        "            else:\r\n",
        "                for x in self.ui_review_dict[user[i]]:\r\n",
        "                    user_reviews.append(self.ui_review_dict[user[i]][x])\r\n",
        "                    user_concepts.append(self.ui_concept_dict[user[i]][x])\r\n",
        "                    user_r_len.append(len(self.ui_review_dict[user[i]][x]))\r\n",
        "                    user_c_len.append(len(self.ui_concept_dict[user[i]][x]))\r\n",
        "                    if len(user_reviews) == self.args.dmax:\r\n",
        "                        break\r\n",
        "                user_list.append(user_reviews)\r\n",
        "                user_concept_list.append(user_concepts)\r\n",
        "                for x in self.iu_review_dict[items[i]]:\r\n",
        "                    item_reviews.append(self.iu_review_dict[items[i]][x])\r\n",
        "                    item_concepts.append(self.iu_concept_dict[items[i]][x])\r\n",
        "                    item_r_len.append(len(self.iu_review_dict[items[i]][x]))\r\n",
        "                    item_c_len.append(len(self.iu_concept_dict[items[i]][x]))\r\n",
        "                    if len(item_reviews) == self.args.dmax:\r\n",
        "                        break\r\n",
        "                item_list.append(item_reviews)\r\n",
        "                item_concept_list.append(item_concepts)\r\n",
        "                user_len.append(user_r_len)\r\n",
        "                item_len.append(item_r_len)\r\n",
        "                user_concept_len.append(user_c_len)\r\n",
        "                item_concept_len.append(item_c_len)\r\n",
        "\r\n",
        "        if(self.args.base_encoder!='Flat'):\r\n",
        "\r\n",
        "            user_concept, user_concept_len = prep_hierarchical_data_list_new(user_concept_list, user_concept_len,\r\n",
        "                                                                                              self.args.smax,\r\n",
        "                                                                                              self.args.dmax)\r\n",
        "            items_concept, item_concept_len = prep_hierarchical_data_list_new(item_concept_list, item_concept_len,\r\n",
        "                                                                                              self.args.smax,\r\n",
        "                                                                                              self.args.dmax)\r\n",
        "\r\n",
        "            user, user_len = prep_hierarchical_data_list_new(user_list, user_len,\r\n",
        "                                                                  self.args.smax,\r\n",
        "                                                                  self.args.dmax)\r\n",
        "            items, item_len = prep_hierarchical_data_list_new(item_list, item_len,\r\n",
        "                                                                   self.args.smax,\r\n",
        "                                                                   self.args.dmax)\r\n",
        "\r\n",
        "        output = [user, user_len, items, item_len]\r\n",
        "\r\n",
        "        output.append(user_concept)\r\n",
        "        output.append(user_concept_len)\r\n",
        "        output.append(items_concept)\r\n",
        "        output.append(item_concept_len)\r\n",
        "        \r\n",
        "        if self.args.implicit == 1:\r\n",
        "            output.append(user_idx)\r\n",
        "            output.append(item_idx)\r\n",
        "      \r\n",
        "        gen_outputs = [x[3] for x in data]\r\n",
        "        gen_len = [x[4] for x in data]\r\n",
        "\r\n",
        "        output.append(gen_outputs)\r\n",
        "        output.append(gen_len)\r\n",
        "        output.append(labels)\r\n",
        "        output = list(zip(*output))\r\n",
        "        return output\r\n",
        "    def infer(self, checkpoint_path):\r\n",
        "        scores = []\r\n",
        "\r\n",
        "        #data = self._prepare_set(self.test_rating_set, self.test_reviews)\r\n",
        "        data = self._combine_reviews(self.test_rating_set, self.test_reviews)\r\n",
        "        num_batches = int(len(data) / self.args.batch_size)\r\n",
        "\r\n",
        "        mkdir_p(self.args.gen_dir)\r\n",
        "        mkdir_p(self.args.gen_true_dir)\r\n",
        "\r\n",
        "        model= Model(self.vocab, self.args, num_user = self.num_user, num_item = self.num_item)\r\n",
        "        model.load_weights(checkpoint_path)\r\n",
        "        data_len = len(data)\r\n",
        "\r\n",
        "        gen_sentences = []\r\n",
        "        ref_sentences = []\r\n",
        "\r\n",
        "        for i in tqdm(range(0, num_batches+1)):\r\n",
        "            batch = batchify(data, i, self.args.batch_size,\r\n",
        "                                max_sample=data_len)\r\n",
        "\r\n",
        "            if(len(batch)==0):\r\n",
        "                    continue\r\n",
        "\r\n",
        "            batch = self._prepare_set(batch)\r\n",
        "            \r\n",
        "            predicted_rating,dec_hidden, word_u, word_i=model(user_reviews, user_len, item_reviews, item_len, \r\n",
        "                                         user_concept, user_concept_len, item_concept, item_concept_len, userid, itemid,label)\r\n",
        "\r\n",
        "            gen_results  = model._beam_search_infer(model.user_output, model.item_output, predicted_rating)\r\n",
        "            gen_results = gen_results[0]\r\n",
        "\r\n",
        "            for j in range(self.args.batch_size):\r\n",
        "                if (self.args.batch_size * i + j)< data_len:\r\n",
        "\r\n",
        "                    f = open(self.args.gen_dir + '/gen_review.'+str(self.args.batch_size * i + j)+'.txt', 'w+')\r\n",
        "                    #for t in xrange(args.beamsize):\r\n",
        "                    new_sentence = []\r\n",
        "                    for k in range(len(gen_results[j*self.args.beam_size])):\r\n",
        "                        if (gen_results[j*self.args.beam_size][k]==self.word_index[EOS]):\r\n",
        "                            break\r\n",
        "                        if k!=0:\r\n",
        "                            f.write(' ')\r\n",
        "                        f.write(self.index_word[gen_results[j*self.args.beam_size][k]])\r\n",
        "                        tmp = self.index_word[gen_results[j*self.args.beam_size][k]].split(' ')\r\n",
        "                        for l in range(len(tmp)):\r\n",
        "                            new_sentence.append(tmp[l])\r\n",
        "                        #new_sentence.append(self.index_word[gen_results[j*self.args.beam_size][k]])\r\n",
        "                        #f.write(' '+str(ppls[j*args.beamsize+t]))\r\n",
        "                        #f.write(' '+str(tags[j*args.beamsize+t]))\r\n",
        "                        #f.write(' '+str(lens[j*args.beamsize+t]))\r\n",
        "                        #f.write('\\n')\r\n",
        "                    f.close()\r\n",
        "                    gen_sentences.append(new_sentence)\r\n",
        "                    #f.write('\\n')\r\n",
        "\r\n",
        "                    #write truth tips\r\n",
        "                    #truth_tip_batch = feed_dict[model.tip_inputs]\r\n",
        "                    new_sentence = []\r\n",
        "                    true_review = self.test_reviews[self.args.batch_size * i + j]\r\n",
        "                    f1 = open(self.args.gen_true_dir + '/true_review.A.'+str(self.args.batch_size * i + j)+'.txt', 'w+')\r\n",
        "                    for k in range(len(true_review)):\r\n",
        "                        if (true_review[k]==self.word_index[EOS]):\r\n",
        "                            break\r\n",
        "                        if k==0:\r\n",
        "                            continue\r\n",
        "                        if k!=1:\r\n",
        "                            f1.write(' ')\r\n",
        "                        f1.write(self.index_word[true_review[k]])\r\n",
        "                        tmp = self.index_word[true_review[k]].split(' ')\r\n",
        "                        for l in range(len(tmp)):\r\n",
        "                            new_sentence.append(tmp[l])\r\n",
        "                    #f1.write('\\n')\r\n",
        "                    f1.close()\r\n",
        "                    ref_sentences.append([new_sentence])\r\n",
        "\r\n",
        "        print ('Infer finished!')\r\n",
        "        score = corpus_bleu(ref_sentences, gen_sentences)\r\n",
        "        print ('bleu score: {}'.format(score))\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        \"\"\" Main training loop\r\n",
        "        \"\"\"\r\n",
        "        scores = []\r\n",
        "        best_score = -1\r\n",
        "        best_dev = -1\r\n",
        "        best_epoch = -1\r\n",
        "        counter = 0\r\n",
        "        min_loss = 1e+7\r\n",
        "        epoch_scores = {}\r\n",
        "        self.eval_list = []\r\n",
        "        data=list(zip(*self.data))\r\n",
        "        a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12,a13=data\r\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((np.array(a1),np.array(a2),np.array(a3),np.array(a4),np.array(a5),\r\n",
        "                                              np.array(a6),np.array(a7),np.array(a8),np.array(a9),\r\n",
        "                                              np.array(a10),np.array(a11),np.array(a12),np.array(a13)))\r\n",
        "        \r\n",
        "        \r\n",
        "        steps=len(a1)//self.args.batch_size \r\n",
        "        loader =dataset.batch(self.args.batch_size)\r\n",
        "        \r\n",
        "  \r\n",
        "        \r\n",
        "        total_loss = []\r\n",
        "        total_loss_n = []\r\n",
        "        total_loss_c = []\r\n",
        "        total_loss_r = []\r\n",
        "        print(\"Training Interactions={}\".format(len(data)))\r\n",
        "        #self.sess.run(tf.assign(self.mdl.is_train,self.mdl.true))\r\n",
        "        optimizer = tf.keras.optimizers.Adam(0.01)\r\n",
        "        checkpoint_path=\"training/cp-{epoch:04d}.ckpt\"\r\n",
        "        min_epoch_loss = float('-inf')\r\n",
        "        for epoch in range(1, self.args.epochs):\r\n",
        "            \r\n",
        "            print('EPOCH:',epoch)\r\n",
        "            all_att_dict = {}\r\n",
        "            pos_val, neg_val = [],[]\r\n",
        "            t0 = time.clock()\r\n",
        "            self.write_to_file(\"=====================================\")\r\n",
        "            losses = []\r\n",
        "            review_losses = []\r\n",
        "            #random.shuffle(data)\r\n",
        "            #num_batches = int(len(data) / self.args.batch_size)\r\n",
        "            norms = []\r\n",
        "            all_acc = 0\r\n",
        "            review_acc = 0\r\n",
        "            user_entropies = []\r\n",
        "            item_entropies = []\r\n",
        "            user_review_hits = []\r\n",
        "            item_review_hits = []\r\n",
        "\r\n",
        "            model = Model(self.vocab, self.args,\r\n",
        "                              num_user = self.num_user, num_item = self.num_item)\r\n",
        "            decoder = Decoder(self.vocab,self.args)\r\n",
        "            epoch_loss=[]\r\n",
        "            epoch_loss_r = []\r\n",
        "            epoch_loss_n=[]\r\n",
        "            epoch_loss_c=[]\r\n",
        "                \r\n",
        "            for user_reviews, user_len, item_reviews, item_len, user_concept, user_concept_len, item_concept, item_concept_len, userid, itemid, gen_output, gen_length, label in loader.take(steps):\r\n",
        "                loss_n = 0\r\n",
        "                key_loss = 0\r\n",
        "                #max_review_length = tf.reduce_max(gen_length)\r\n",
        "                #masks = tf.transpose(tf.sequence_mask(gen_length, maxlen=max_review_length, dtype=tf.float32), perm=[1,0])\r\n",
        "                #print('Gen_Output:')\r\n",
        "                #print(gen_output)\r\n",
        "                with tf.GradientTape() as tape:\r\n",
        "                  predicted_rating,dec_hidden, word_u, word_i=model(user_reviews, user_len, item_reviews, item_len, \r\n",
        "                                         user_concept, user_concept_len, item_concept, item_concept_len, userid, itemid,label)\r\n",
        "                  #dec_input = tf.expand_dims([2] * self.args.batch_size, 1)\r\n",
        "                  dec_input = tf.expand_dims(gen_output[:,0],1)\r\n",
        "                  for t in range(1,gen_output.shape[1]):\r\n",
        "                    predictions, dec_hidden = decoder(dec_input,dec_hidden)\r\n",
        "                    loss_n += loss_function(gen_output[:,t],predictions)\r\n",
        "                    key_loss+= _cal_key_loss(self.args, predictions, word_u, word_i)\r\n",
        "                    #using teacher forcing\r\n",
        "                    dec_input = tf.expand_dims(gen_output[:,t],1)\r\n",
        "                  \r\n",
        "                  #variables = model.trainable_variables + decoder.trainable_variables\r\n",
        "                  \r\n",
        "                  loss = tf.keras.losses.MSE(label, tf.reshape(predicted_rating, [-1]))\r\n",
        "                  epoch_loss_r.append(loss/int(gen_output.shape[1]))\r\n",
        "                  loss += loss_n+key_loss\r\n",
        "                  epoch_loss.append(loss)\r\n",
        "                  epoch_loss_n.append(loss_n/int(gen_output.shape[1]))\r\n",
        "                  epoch_loss_c.append(key_loss/int(gen_output.shape[1]))\r\n",
        "                  \r\n",
        "                variables = model.trainable_variables + decoder.trainable_variables\r\n",
        "                gradients = tape.gradient(loss, variables)\r\n",
        "                #print('GRADIENTS')\r\n",
        "                #print(gradients)\r\n",
        "                optimizer.apply_gradients(zip(gradients,variables))\r\n",
        "            #print()\r\n",
        "            print()\r\n",
        "            print('Total_loss in epoch', epoch-1)\r\n",
        "            print(sum(epoch_loss))\r\n",
        "            print('Cross entropy loss')\r\n",
        "            print(sum(epoch_loss_r))\r\n",
        "            print('Generation loss:')\r\n",
        "            print(sum(epoch_loss_n))\r\n",
        "            print('Key loss:')\r\n",
        "            print(sum(epoch_loss_c))\r\n",
        "            \r\n",
        "            total_loss.append(sum(epoch_loss))\r\n",
        "            total_loss_r.append(sum(epoch_loss_r))\r\n",
        "            total_loss_n.append(sum(epoch_loss_n))\r\n",
        "            total_loss_c.append(sum(epoch_loss_c))\r\n",
        "            if sum(epoch_loss) < min_epoch_loss:\r\n",
        "              model.save_weights(checkpoint_path.format(epoch=epoch-1))\r\n",
        "              model.load_weights(checkpoint_path.format(epoch=epoch-1))\r\n",
        "              min_epoch_loss=sum(epoch_loss)\r\n",
        "        print(total_loss)\r\n",
        "        plt.ioff()\r\n",
        "        plt.plot(total_loss_r)\r\n",
        "        plt.savefig(\"mseloss.png\")\r\n",
        "        plt.plot(total_loss)\r\n",
        "        plt.savefig(\"totalloss.png\")\r\n",
        "        fig = plt.figure()\r\n",
        "        plt.plot(total_loss_n)\r\n",
        "        plt.savefig(\"crossentropylossreview.png\")\r\n",
        "        fig = plt.figure()        \r\n",
        "        plt.plot(total_loss_c)\r\n",
        "        plt.savefig(\"conceptrelevanceloss.png\")\r\n",
        "        fig = plt.figure()\r\n",
        "               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh5DYmwdU0If",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9f2c7a-8171-48b5-fccf-317ad924874c"
      },
      "source": [
        "c=CFExperiment(preprocessed=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Rec Experiment\n",
            "Setting up environment..\n",
            "Selecting GPU no.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh4S8uf9a1Fz"
      },
      "source": [
        "# Run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxTLaHya-gfb"
      },
      "source": [
        "c.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}